#!/usr/bin/env python3
"""
ç°¡å–®çš„ wrapperï¼šä½¿ç”¨åŸç‰ˆ infer.pyï¼Œä½†åªè™•ç†å‰ N å€‹æ¨£æœ¬

é€™å€‹è…³æœ¬æœƒï¼š
1. ç›´æ¥èª¿ç”¨åŸç‰ˆ infer.py çš„å‡½æ•¸
2. åœ¨ DataLoader å±¤é¢é™åˆ¶æ¨£æœ¬æ•¸é‡
3. é¿å…ä¸‹è¼‰æ•´å€‹ 100GB+ datasetï¼ˆé›–ç„¶é¦–æ¬¡ä»æœƒä¸‹è¼‰ metadataï¼‰

ä½¿ç”¨æ–¹å¼ï¼š
    cd /home/hpc/ce505203/finals_repo/third_party/cld/infer
    python ../../../scripts/test_original_cld_limited.py --config_path <config.yaml> --max_samples 5
"""

import sys
import os
from pathlib import Path
import argparse

# å…ˆæª¢æŸ¥ CUDA å¯ç”¨æ€§ï¼ˆåœ¨å°å…¥ infer.py ä¹‹å‰ï¼‰
# å› ç‚º infer.py æœƒè¨­ç½® CUDA_VISIBLE_DEVICES = "1"ï¼Œæˆ‘å€‘éœ€è¦å…ˆè™•ç†
import torch

# è¨­å®šè·¯å¾‘
script_path = Path(__file__).resolve()
repo_root = script_path.parent.parent
cld_root = repo_root / "third_party" / "cld"
cld_infer_dir = cld_root / "infer"

sys.path.insert(0, str(cld_root))
os.chdir(str(cld_root))

# æª¢æŸ¥ CUDA å¯ç”¨æ€§
cuda_available = torch.cuda.is_available()
if cuda_available:
    device_count = torch.cuda.device_count()
    print(f"âœ… CUDA å¯ç”¨ï¼š{torch.cuda.get_device_name(0)}")
    print(f"   CUDA ç‰ˆæœ¬ï¼š{torch.version.cuda}")
    print(f"   å¯ç”¨ GPU æ•¸é‡ï¼š{device_count}\n")
else:
    print("\n" + "="*60)
    print("âš ï¸  è­¦å‘Šï¼šCUDA ä¸å¯ç”¨ï¼")
    print("="*60)
    print("CLD inference éœ€è¦ GPU æ‰èƒ½é‹è¡Œã€‚")
    print("å¦‚æœæ²’æœ‰ GPUï¼Œæ¨ç†æœƒéå¸¸æ…¢ï¼ˆå¯èƒ½éœ€è¦æ•¸å°æ™‚ï¼‰ã€‚")
    print("\nå»ºè­°ï¼š")
    print("1. ç¢ºä¿ GPU å¯ç”¨ï¼šnvidia-smi")
    print("2. ç¢ºä¿ CUDA_VISIBLE_DEVICES è¨­ç½®æ­£ç¢º")
    print("3. ç¢ºä¿ PyTorch å®‰è£äº† CUDA æ”¯æŒ")
    print("="*60 + "\n")
    
    response = input("æ˜¯å¦ç¹¼çºŒä½¿ç”¨ CPUï¼Ÿï¼ˆy/Nï¼‰: ")
    if response.lower() != 'y':
        print("å·²å–æ¶ˆã€‚")
        raise SystemExit(1)

# Memory optimization: ä½¿ç”¨é¡¯å¼åŠ è¼‰ä¾†å¯¦ç¾ T5 NF4 é‡åŒ–å’Œå…¶ä»–æ¨¡å‹çš„ bfloat16 å„ªåŒ–
print("[INFO] æ‡‰ç”¨è¨˜æ†¶é«”å„ªåŒ–ï¼šT5 NF4é‡åŒ– + å…¶ä»–æ¨¡å‹ bfloat16 + safetensors...", flush=True)

# æº–å‚™ T5 NF4 é‡åŒ–é…ç½®
t5_quantization_config = None
try:
    from transformers import BitsAndBytesConfig
    t5_quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,  # ä½¿ç”¨é›™é‡é‡åŒ–ä»¥é€²ä¸€æ­¥ç¯€çœè¨˜æ†¶é«”
    )
    print("âœ… T5 NF4 é‡åŒ–é…ç½®æº–å‚™å®Œæˆï¼ˆ4-bit + double quantizationï¼‰", flush=True)
    print("ğŸ’¾ T5 è¨˜æ†¶é«”é æœŸï¼š~10GB â†’ ~4.8GBï¼ˆç¯€çœ ~50%ï¼‰", flush=True)
except ImportError:
    print("âš ï¸  Warning: bitsandbytes æœªå®‰è£ï¼Œç„¡æ³•ä½¿ç”¨ T5 NF4 é‡åŒ–", flush=True)
    print("   å®‰è£æ–¹å¼: pip install bitsandbytes", flush=True)
    print("   T5 å°‡ä½¿ç”¨ bfloat16 åŠ è¼‰ï¼ˆè¨˜æ†¶é«”ä½¿ç”¨è¼ƒé«˜ï¼‰", flush=True)

# ===== æ·»åŠ å…¶ä»–æ¨¡å‹çš„ bfloat16 + safetensors å„ªåŒ– =====
print("[INFO] æ‡‰ç”¨å…¶ä»–æ¨¡å‹çš„è¨˜æ†¶é«”å„ªåŒ– patchï¼šbfloat16 + safetensors...", flush=True)
try:
    from diffusers import ModelMixin

    # Store original from_pretrained method (it's already a classmethod)
    original_modelmixin_from_pretrained_func = ModelMixin.from_pretrained.__func__

    def patched_from_pretrained(cls, pretrained_model_name_or_path, *args, **kwargs):
        """Patched from_pretrained that enforces memory optimizations for non-T5 models."""
        # Force torch_dtype=bfloat16 if not specified
        if 'torch_dtype' not in kwargs:
            kwargs['torch_dtype'] = torch.bfloat16

        # Force low_cpu_mem_usage=True
        if 'low_cpu_mem_usage' not in kwargs:
            kwargs['low_cpu_mem_usage'] = True

        # Prefer safetensors if available (enables memory mapping)
        if 'use_safetensors' not in kwargs:
            kwargs['use_safetensors'] = True

        # Call original method with correct signature
        model = original_modelmixin_from_pretrained_func(cls, pretrained_model_name_or_path, *args, **kwargs)

        # ç¢ºä¿æ¨¡å‹åœ¨ GPU ä¸Šï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if torch.cuda.is_available() and model is not None:
            try:
                device = next(model.parameters()).device if hasattr(model, 'parameters') else None
                if device is not None and device.type != 'cuda':
                    model = model.to('cuda')
                    # ç¢ºä¿æ‰€æœ‰åƒæ•¸éƒ½åœ¨ GPU ä¸Š
                    if hasattr(model, 'named_parameters'):
                        for name, param in model.named_parameters():
                            if param.device.type != 'cuda':
                                param.data = param.data.to('cuda')
                    # ç¢ºä¿æ‰€æœ‰ buffers éƒ½åœ¨ GPU ä¸Š
                    if hasattr(model, 'named_buffers'):
                        for name, buffer in model.named_buffers():
                            if buffer.device.type != 'cuda':
                                buffer.data = buffer.data.to('cuda')
                    torch.cuda.synchronize()
            except Exception as e:
                print(f"  âš ï¸  Warning: Could not move {cls.__name__} to GPU: {e}", flush=True)

        return model

    # Apply monkey patch as classmethod
    ModelMixin.from_pretrained = classmethod(patched_from_pretrained)
    print("âœ… å·²æ‡‰ç”¨å…¶ä»–æ¨¡å‹è¨˜æ†¶é«”å„ªåŒ– patchï¼štorch_dtype=bfloat16, low_cpu_mem_usage=True, use_safetensors=True, GPU loading", flush=True)

except ImportError as e:
    print(f"âš ï¸  Warning: Could not apply memory optimization patches: {e}", flush=True)
    print("   Model loading may use more memory than necessary.", flush=True)
except Exception as e:
    print(f"âš ï¸  Warning: Error applying memory optimization patches: {e}", flush=True)

# å°å…¥åŸç‰ˆ infer.py
# æ³¨æ„ï¼šinfer.py æœƒè¨­ç½® CUDA_VISIBLE_DEVICES = "1"
# å¦‚æœç³»çµ±åªæœ‰ GPU 0ï¼Œé€™æœƒå°è‡´å•é¡Œï¼Œæ‰€ä»¥æˆ‘å€‘éœ€è¦ä¿®æ”¹ä»£ç¢¼
import importlib.util
infer_py_path = cld_infer_dir / "infer.py"

# è®€å– infer.py çš„ä»£ç¢¼
with open(infer_py_path, 'r', encoding='utf-8') as f:
    infer_code = f.read()

# å¦‚æœåªæœ‰ä¸€å€‹ GPUï¼Œä¿®æ”¹ CUDA_VISIBLE_DEVICES è¨­ç½®
if cuda_available and device_count == 1:
    # å°‡ CUDA_VISIBLE_DEVICES = "1" æ”¹ç‚º "0" æˆ–ç§»é™¤
    if 'os.environ["CUDA_VISIBLE_DEVICES"] = "1"' in infer_code:
        infer_code = infer_code.replace(
            'os.environ["CUDA_VISIBLE_DEVICES"] = "1"',
            'os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # ä¿®æ”¹ç‚ºä½¿ç”¨ GPU 0'
        )
        print("[INFO] æª¢æ¸¬åˆ°åªæœ‰ä¸€å€‹ GPUï¼Œå·²ä¿®æ”¹ infer.py ä¸­çš„ CUDA_VISIBLE_DEVICES=1 â†’ 0\n")

# å‰µå»ºæ¨¡çµ„ä¸¦åŸ·è¡Œä¿®æ”¹å¾Œçš„ä»£ç¢¼
spec = importlib.util.spec_from_file_location("infer_module", str(infer_py_path))
infer_module = importlib.util.module_from_spec(spec)
# åŸ·è¡Œä¿®æ”¹å¾Œçš„ä»£ç¢¼
exec(compile(infer_code, str(infer_py_path), 'exec'), infer_module.__dict__)

from torch.utils.data import Subset, Dataset

# Optimize LoRA loading: Monkey patch CustomFluxPipeline.lora_state_dict AFTER loading infer_module
# This ensures LoRA weights are loaded directly to GPU using safetensors for faster loading
print("[DEBUG] Starting LoRA optimization setup...", flush=True)
CustomFluxPipeline = None  # Will be set in try block
try:
    import time
    
    # Import CustomFluxPipeline from the loaded module
    # This import may be slow if models.pipeline is large
    print("[DEBUG] Importing CustomFluxPipeline (this may take a moment)...", flush=True)
    import_start = time.time()
    from models.pipeline import CustomFluxPipeline
    import_elapsed = time.time() - import_start
    print(f"[DEBUG] CustomFluxPipeline imported in {import_elapsed:.2f}s", flush=True)
    
    # Store original lora_state_dict method
    print("[DEBUG] Checking for lora_state_dict method...", flush=True)
    if hasattr(CustomFluxPipeline, 'lora_state_dict'):
        print("[DEBUG] Found lora_state_dict, creating optimized version...", flush=True)
        original_lora_state_dict = CustomFluxPipeline.lora_state_dict
        
        @staticmethod
        def optimized_lora_state_dict(lora_path, *args, **kwargs):
            """
            Optimized LoRA loading that:
            1. Ensures safetensors format is used
            2. Loads directly to GPU (cuda)
            3. Provides progress indication
            
            Returns:
                - If called by load_lora_into_transformer: state_dict only
                - If called by pipeline.load_lora_weights(): (state_dict, network_alphas) tuple
            """
            import safetensors.torch
            
            lora_path_obj = Path(lora_path)
            
            # Check if it's a directory or file path
            if lora_path_obj.is_dir():
                # Look for safetensors file in directory
                safetensors_file = lora_path_obj / "pytorch_lora_weights.safetensors"
                if not safetensors_file.exists():
                    # Fallback: try to find any safetensors file
                    safetensors_files = list(lora_path_obj.glob("*.safetensors"))
                    if safetensors_files:
                        safetensors_file = safetensors_files[0]
                    else:
                        print(f"âš ï¸  Warning: No safetensors file found in {lora_path}, falling back to original method")
                        return original_lora_state_dict(lora_path, *args, **kwargs)
                lora_file = safetensors_file
            elif lora_path_obj.suffix == ".safetensors":
                lora_file = lora_path_obj
            else:
                # Not safetensors format, try to find safetensors version
                safetensors_file = lora_path_obj.parent / f"{lora_path_obj.stem}.safetensors"
                if safetensors_file.exists():
                    lora_file = safetensors_file
                    print(f"   ğŸ“¦ Found safetensors version: {lora_file}")
                else:
                    print(f"âš ï¸  Warning: LoRA file is not safetensors format: {lora_path}")
                    print(f"   Expected safetensors file: {safetensors_file}")
                    print(f"   Falling back to original method (may be slower)")
                    return original_lora_state_dict(lora_path, *args, **kwargs)
            
            if not lora_file.exists():
                print(f"âš ï¸  Warning: LoRA file not found: {lora_file}, falling back to original method")
                return original_lora_state_dict(lora_path, *args, **kwargs)
            
            print(f"   ğŸ“¦ Loading LoRA weights from: {lora_file}", flush=True)
            print(f"   âœ… Using safetensors format (faster loading)", flush=True)
            
            start_time = time.time()
            
            # Determine device - prefer GPU if available
            device = "cuda" if torch.cuda.is_available() else "cpu"
            if device == "cpu":
                print(f"   âš ï¸  Warning: CUDA not available, loading to CPU (will be slower)", flush=True)
            
            try:
                # Load safetensors - try direct GPU loading first, fallback to CPU then move to GPU
                # safetensors.torch.load_file returns a dict of tensors
                load_start = time.time()
                try:
                    # Try loading directly to GPU if device parameter is supported
                    state_dict = safetensors.torch.load_file(str(lora_file), device=device)
                    load_elapsed = time.time() - load_start
                    loaded_to_gpu = (device == "cuda")
                    print(f"   â±ï¸  Safetensors I/O: {load_elapsed:.2f}s", flush=True)
                except TypeError:
                    # device parameter not supported, load to CPU then move to GPU
                    state_dict = safetensors.torch.load_file(str(lora_file))
                    load_elapsed = time.time() - load_start
                    print(f"   â±ï¸  Safetensors I/O (CPU): {load_elapsed:.2f}s", flush=True)
                    
                    if device == "cuda" and torch.cuda.is_available():
                        # Move all tensors to GPU efficiently
                        # Batch move for better performance (avoids multiple small transfers)
                        move_start = time.time()
                        # Collect all tensors first, then move them in batch
                        tensor_keys = [k for k, v in state_dict.items() if isinstance(v, torch.Tensor)]
                        if tensor_keys:
                            # Use non_blocking=True for async transfer, but we'll sync at the end
                            for k in tensor_keys:
                                state_dict[k] = state_dict[k].cuda(non_blocking=True)
                            # Synchronize to ensure all transfers complete before returning
                            torch.cuda.synchronize()
                        move_elapsed = time.time() - move_start
                        print(f"   â±ï¸  CPU->GPU transfer ({len(tensor_keys)} tensors): {move_elapsed:.2f}s", flush=True)
                        loaded_to_gpu = True
                    else:
                        loaded_to_gpu = False
                
                elapsed = time.time() - start_time
                num_keys = len(state_dict)
                file_size_mb = lora_file.stat().st_size / (1024 * 1024)
                
                print(f"   âœ… LoRA loaded ({num_keys} keys, {file_size_mb:.2f} MB) in {elapsed:.2f}s total", flush=True)
                if loaded_to_gpu:
                    print(f"   ğŸš€ Loaded directly to GPU (optimized path)", flush=True)
                
                # Always return tuple (state_dict, network_alphas) as expected by load_lora_weights
                # load_lora_into_transformer wrapper will handle unpacking
                network_alphas = None
                return state_dict, network_alphas
                
            except Exception as e:
                print(f"   âš ï¸  Error loading safetensors: {e}", flush=True)
                print(f"   Falling back to original method", flush=True)
                return original_lora_state_dict(lora_path, *args, **kwargs)
        
        # Apply monkey patch
        print("[DEBUG] About to apply monkey patch to CustomFluxPipeline.lora_state_dict...", flush=True)
        CustomFluxPipeline.lora_state_dict = optimized_lora_state_dict
        print("[DEBUG] Monkey patch applied successfully", flush=True)
        print("âœ… Applied LoRA loading optimization: safetensors + direct GPU loading", flush=True)
        print("[DEBUG] Finished LoRA optimization setup", flush=True)
        
        # Also patch load_lora_into_transformer to handle tuple return from optimized_lora_state_dict
        if hasattr(CustomFluxPipeline, 'load_lora_into_transformer'):
            original_load_lora = CustomFluxPipeline.load_lora_into_transformer
            
            @staticmethod
            def timed_load_lora_into_transformer(lora_state_dict, *args, **kwargs):
                print("[DEBUG] load_lora_into_transformer: Starting...", flush=True)
                start = time.time()
                
                # Handle case where lora_state_dict might be a tuple (from optimized_lora_state_dict)
                # load_lora_into_transformer expects just the state_dict, not the tuple
                if isinstance(lora_state_dict, tuple):
                    lora_state_dict, _ = lora_state_dict  # Unpack tuple, ignore network_alphas
                
                result = original_load_lora(lora_state_dict, *args, **kwargs)
                elapsed = time.time() - start
                print(f"[DEBUG] load_lora_into_transformer: Completed in {elapsed:.2f}s", flush=True)
                return result
            
            CustomFluxPipeline.load_lora_into_transformer = timed_load_lora_into_transformer
        
except ImportError as e:
    print(f"âš ï¸  Warning: Could not apply LoRA loading optimization: {e}")
    print("   LoRA loading will use default method (may be slower)")
except Exception as e:
    print(f"âš ï¸  Warning: Error applying LoRA loading optimization: {e}")
    print("   LoRA loading will use default method (may be slower)")

# Optimize fuse_lora to run on GPU with better performance
print("[INFO] Optimizing fuse_lora for GPU execution...", flush=True)
try:
    from models.mmdit import CustomFluxTransformer2DModel
    from models.multiLayer_adapter import MultiLayerAdapter
    
    # Store original fuse_lora methods
    if hasattr(CustomFluxTransformer2DModel, 'fuse_lora'):
        original_fuse_lora_transformer = CustomFluxTransformer2DModel.fuse_lora
        
        def optimized_fuse_lora_transformer(self, *args, **kwargs):
            """
            Optimized fuse_lora that ensures:
            1. Model is on GPU
            2. All operations run on GPU
            3. Proper synchronization
            4. Timing information
            """
            import time
            print("[DEBUG] fuse_lora (Transformer): Starting GPU-optimized fusion...", flush=True)
            
            # Ensure model is on GPU
            device = next(self.parameters()).device
            if device.type != 'cuda':
                print(f"  âš ï¸  Warning: Model is on {device}, moving to GPU...", flush=True)
                self.to('cuda')
                device = 'cuda'
            else:
                print(f"  âœ… Model is on GPU: {device}", flush=True)
            
            # Ensure all parameters are on GPU
            for name, param in self.named_parameters():
                if param.device.type != 'cuda':
                    print(f"  âš ï¸  Moving parameter {name} to GPU...", flush=True)
                    param.data = param.data.to('cuda')
            
            # Clear cache before fusion
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            
            start_time = time.time()
            
            # Call original fuse_lora
            result = original_fuse_lora_transformer(self, *args, **kwargs)
            
            # Synchronize to ensure all GPU operations complete
            if torch.cuda.is_available():
                torch.cuda.synchronize()
            
            elapsed = time.time() - start_time
            print(f"  âœ… fuse_lora (Transformer): Completed in {elapsed:.2f}s on GPU", flush=True)
            
            return result
        
        CustomFluxTransformer2DModel.fuse_lora = optimized_fuse_lora_transformer
        print("  âœ… Optimized CustomFluxTransformer2DModel.fuse_lora", flush=True)
    
    if hasattr(MultiLayerAdapter, 'fuse_lora'):
        original_fuse_lora_adapter = MultiLayerAdapter.fuse_lora
        
        def optimized_fuse_lora_adapter(self, *args, **kwargs):
            """
            Optimized fuse_lora for MultiLayerAdapter that ensures GPU execution
            """
            import time
            print("[DEBUG] fuse_lora (MultiLayerAdapter): Starting GPU-optimized fusion...", flush=True)
            
            # Ensure model is on GPU
            device = next(self.parameters()).device
            if device.type != 'cuda':
                print(f"  âš ï¸  Warning: MultiLayerAdapter is on {device}, moving to GPU...", flush=True)
                self.to('cuda')
                device = 'cuda'
            else:
                print(f"  âœ… MultiLayerAdapter is on GPU: {device}", flush=True)
            
            # Ensure all parameters are on GPU
            for name, param in self.named_parameters():
                if param.device.type != 'cuda':
                    print(f"  âš ï¸  Moving parameter {name} to GPU...", flush=True)
                    param.data = param.data.to('cuda')
            
            # Clear cache before fusion
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            
            start_time = time.time()
            
            # Call original fuse_lora
            result = original_fuse_lora_adapter(self, *args, **kwargs)
            
            # Synchronize to ensure all GPU operations complete
            if torch.cuda.is_available():
                torch.cuda.synchronize()
            
            elapsed = time.time() - start_time
            print(f"  âœ… fuse_lora (MultiLayerAdapter): Completed in {elapsed:.2f}s on GPU", flush=True)
            
            return result
        
        MultiLayerAdapter.fuse_lora = optimized_fuse_lora_adapter
        print("  âœ… Optimized MultiLayerAdapter.fuse_lora", flush=True)
    
    print("âœ… GPU-optimized fuse_lora patches applied", flush=True)
except ImportError as e:
    print(f"âš ï¸  Warning: Could not optimize fuse_lora: {e}", flush=True)
except Exception as e:
    print(f"âš ï¸  Warning: Error optimizing fuse_lora: {e}", flush=True)
    import traceback
    traceback.print_exc()


def apply_skip_fuse_lora_patch(config):
    """
    å¦‚æœ config ä¸­è¨­ç½®äº† skip_fuse_lora=Trueï¼Œå‰‡ patch fuse_lora å’Œ unload_lora ç‚º no-op
    é€™æ¨£å¯ä»¥ç¯€çœè¨˜æ†¶é«”ï¼ŒLoRA æœƒé€šé PEFT æ©Ÿåˆ¶å·¥ä½œï¼ˆæ€§èƒ½æå¤± <5%ï¼‰
    """
    skip_fuse_lora = config.get('skip_fuse_lora', False)
    
    if not skip_fuse_lora:
        return
    
    print("âš ï¸  skip_fuse_lora=True: Will skip fuse_lora() to save memory", flush=True)
    print("   LoRA will work via PEFT mechanism (slight performance loss <5%, but saves memory)", flush=True)
    
    # Monkey patch fuse_lora and unload_lora to be no-ops
    if CustomFluxPipeline is not None:
        try:
            from models.mmdit import CustomFluxTransformer2DModel
            from models.multiLayer_adapter import MultiLayerAdapter
            
            # Patch fuse_lora to be a no-op
            if hasattr(CustomFluxTransformer2DModel, 'fuse_lora'):
                def noop_fuse_lora(self, *args, **kwargs):
                    print("[INFO] Skipping fuse_lora() to save memory (LoRA will work via PEFT)", flush=True)
                    return None
                CustomFluxTransformer2DModel.fuse_lora = noop_fuse_lora
            
            if hasattr(MultiLayerAdapter, 'fuse_lora'):
                def noop_fuse_lora_adapter(self, *args, **kwargs):
                    print("[INFO] Skipping MultiLayerAdapter.fuse_lora() to save memory", flush=True)
                    return None
                MultiLayerAdapter.fuse_lora = noop_fuse_lora_adapter
            
            # Patch unload_lora to be a no-op (since we didn't fuse, we shouldn't unload)
            if hasattr(CustomFluxTransformer2DModel, 'unload_lora'):
                def noop_unload_lora(self, *args, **kwargs):
                    print("[INFO] Skipping unload_lora() (LoRA weights kept for PEFT inference)", flush=True)
                    return None
                CustomFluxTransformer2DModel.unload_lora = noop_unload_lora
            
            if hasattr(MultiLayerAdapter, 'unload_lora'):
                def noop_unload_lora_adapter(self, *args, **kwargs):
                    print("[INFO] Skipping MultiLayerAdapter.unload_lora()", flush=True)
                    return None
                MultiLayerAdapter.unload_lora = noop_unload_lora_adapter
            
            print("âœ… Patched fuse_lora/unload_lora to skip (memory optimization)", flush=True)
        except ImportError:
            print("âš ï¸  Warning: Could not patch fuse_lora/unload_lora (models not available)", flush=True)
        except Exception as e:
            print(f"âš ï¸  Warning: Error patching fuse_lora/unload_lora: {e}", flush=True)


class LimitedLayoutTrainDataset(Dataset):
    """
    ä¿®æ”¹ç‰ˆçš„ LayoutTrainDatasetï¼Œåœ¨åˆå§‹åŒ–æ™‚å°±é™åˆ¶æ¨£æœ¬æ•¸é‡
    é€™æ¨£å¯ä»¥é¿å…è™•ç†æ•´å€‹ datasetï¼ˆé›–ç„¶ metadata é‚„æ˜¯æœƒä¸‹è¼‰ï¼‰
    """
    def __init__(self, data_dir, split="test", max_samples=None):
        from datasets import load_dataset, concatenate_datasets
        from collections import defaultdict
        import numpy as np
        import torchvision.transforms as T
        from PIL import Image
        
        print(f"[INFO] åŠ è¼‰ PrismLayersPro datasetï¼ˆsplit={split}ï¼‰...", flush=True)
        print(f"[INFO] ä½¿ç”¨ streaming æ¨¡å¼ä»¥é¿å…ä¸‹è¼‰æ•´å€‹æ•¸æ“šé›†å…ƒæ•¸æ“š", flush=True)
        print(f"[INFO] åœ–ç‰‡æœƒæŒ‰éœ€ä¸‹è¼‰ï¼Œåªæœƒä¸‹è¼‰å¯¦éš›è¨ªå•çš„æ¨£æœ¬", flush=True)
        
        # ä½¿ç”¨ streaming=True ä¾†é¿å…ä¸‹è¼‰æ•´å€‹æ•¸æ“šé›†çš„å…ƒæ•¸æ“š
        # é€™å°æ–¼å¤§å‹æ•¸æ“šé›†éå¸¸é‡è¦ï¼Œå¯ä»¥ç¯€çœå¤§é‡æ™‚é–“å’Œç©ºé–“
        streaming_dataset = load_dataset(
            "artplus/PrismLayersPro",
            cache_dir=data_dir,
            streaming=True,  # å•Ÿç”¨æµå¼åŠ è¼‰ï¼Œé¿å…ä¸‹è¼‰æ‰€æœ‰å…ƒæ•¸æ“š
        )
        
        # å°æ–¼å°æ¨£æœ¬æ¸¬è©¦ï¼ˆmax_samples å¾ˆå°ï¼‰ï¼Œç°¡åŒ–é‚è¼¯ï¼š
        # ç›´æ¥å¾ streaming dataset ä¸­å–æ¨£æœ¬ï¼Œè·³éè¤‡é›œçš„ style_category åˆ†çµ„
        if max_samples is not None and max_samples > 0 and max_samples < 100:
            print(f"[INFO] å°æ¨£æœ¬æ¨¡å¼ï¼šç›´æ¥å¾ streaming dataset å–å‰ {max_samples} å€‹æ¨£æœ¬", flush=True)
            print(f"[INFO] è·³é style_category åˆ†çµ„ä»¥åŠ å¿«åŠ è¼‰é€Ÿåº¦", flush=True)
            
            # åˆä½µæ‰€æœ‰ splits çš„æµå¼æ•¸æ“šé›†
            from itertools import islice, chain
            all_streams = []
            for split_name, split_dataset in streaming_dataset.items():
                all_streams.append(split_dataset)
            
            # åˆä½µæ‰€æœ‰æµä¸¦å–å‰ max_samples å€‹æ¨£æœ¬
            combined_stream = chain(*all_streams)
            limited_items = list(islice(combined_stream, max_samples))
            
            # è½‰æ›ç‚ºå¯ç´¢å¼•çš„ Dataset
            from datasets import Dataset
            self.dataset = Dataset.from_list(limited_items)
            print(f"[INFO] âœ… å·²åŠ è¼‰ {len(self.dataset)} å€‹æ¨£æœ¬ï¼ˆä½¿ç”¨ streaming æ¨¡å¼ï¼‰", flush=True)
        else:
            # å°æ–¼å¤§æ¨£æœ¬æˆ–éœ€è¦å®Œæ•´åˆ†çµ„çš„æƒ…æ³ï¼Œéœ€è¦æ”¶é›†è¶³å¤ çš„æ¨£æœ¬ä¾†é€²è¡Œåˆ†çµ„
            # ç‚ºäº†é€²è¡Œ style_category åˆ†çµ„ï¼Œæˆ‘å€‘éœ€è¦æ”¶é›†æ¯” max_samples æ›´å¤šçš„æ¨£æœ¬
            sample_multiplier = 10 if max_samples else 1
            target_samples = (max_samples * sample_multiplier) if max_samples else None
            
            print(f"[INFO] æ”¶é›†æ¨£æœ¬ä»¥é€²è¡Œ style_category åˆ†çµ„...", flush=True)
            if target_samples:
                print(f"[INFO] ç›®æ¨™æ”¶é›† {target_samples} å€‹æ¨£æœ¬ï¼ˆç”¨æ–¼åˆ†çµ„ï¼‰", flush=True)
            
            # æ”¶é›†æ¨£æœ¬
            from itertools import islice, chain
            all_streams = []
            for split_name, split_dataset in streaming_dataset.items():
                all_streams.append(split_dataset)
            
            combined_stream = chain(*all_streams)
            if target_samples:
                collected_items = list(islice(combined_stream, target_samples))
            else:
                # å¦‚æœæ²’æœ‰ max_samples é™åˆ¶ï¼Œæ”¶é›†æ‰€æœ‰æ¨£æœ¬ï¼ˆé€™å¯èƒ½æœƒå¾ˆæ…¢ï¼‰
                print(f"[INFO] âš ï¸  è­¦å‘Šï¼šæ²’æœ‰ max_samples é™åˆ¶ï¼Œå°‡æ”¶é›†æ‰€æœ‰æ¨£æœ¬ï¼ˆé€™å¯èƒ½éœ€è¦å¾ˆé•·æ™‚é–“ï¼‰", flush=True)
                collected_items = list(combined_stream)
            
            # è½‰æ›ç‚ºå¯ç´¢å¼•çš„ Dataset
            from datasets import Dataset
            full_dataset = Dataset.from_list(collected_items)
            print(f"[INFO] å·²æ”¶é›† {len(full_dataset)} å€‹æ¨£æœ¬", flush=True)

            if "style_category" not in full_dataset.column_names:
                raise ValueError("Dataset must contain a 'style_category' field to split by class.")

            categories = np.array(full_dataset["style_category"])
            category_to_indices = defaultdict(list)
            for i, cat in enumerate(categories):
                category_to_indices[cat].append(i)

            subsets = []
            for cat, indices in category_to_indices.items():
                total_len = len(indices)
                idx_90 = int(total_len * 0.9)
                idx_95 = int(total_len * 0.95)

                if split == "train":
                    selected_idx = indices[:idx_90]
                elif split == "test":
                    selected_idx = indices[idx_90:idx_95]
                elif split == "val":
                    selected_idx = indices[idx_95:]
                else:
                    raise ValueError("split must be 'train', 'val', or 'test'")

                subsets.append(full_dataset.select(selected_idx))

            # åˆä½µæ‰€æœ‰ subsets
            combined_dataset = concatenate_datasets(subsets)
            
            # åœ¨åˆå§‹åŒ–æ™‚å°±é™åˆ¶æ¨£æœ¬æ•¸é‡
            if max_samples is not None and max_samples > 0:
                actual_samples = min(max_samples, len(combined_dataset))
                print(f"[INFO] é™åˆ¶æ¨£æœ¬æ•¸é‡ï¼š{len(combined_dataset)} â†’ {actual_samples}", flush=True)
                self.dataset = combined_dataset.select(range(actual_samples))
            else:
                self.dataset = combined_dataset
        
        self.to_tensor = T.ToTensor()

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        item = self.dataset[idx]

        def rgba2rgb(img_RGBA):
            from PIL import Image
            img_RGB = Image.new("RGB", img_RGBA.size, (128, 128, 128))
            img_RGB.paste(img_RGBA, mask=img_RGBA.split()[3])
            return img_RGB

        def get_img(x):
            from PIL import Image
            if isinstance(x, str):
                img_RGBA = Image.open(x).convert("RGBA")
                img_RGB = rgba2rgb(img_RGBA)
            else:
                img_RGBA = x.convert("RGBA")
                img_RGB = rgba2rgb(img_RGBA)
            return img_RGBA, img_RGB

        whole_img_RGBA, whole_img_RGB = get_img(item["whole_image"])
        whole_cap = item["whole_caption"]
        W, H = whole_img_RGBA.size
        base_layout = [0, 0, W - 1, H - 1]

        layer_image_RGBA = [self.to_tensor(whole_img_RGBA)]
        layer_image_RGB  = [self.to_tensor(whole_img_RGB)]
        layout = [base_layout]

        base_img_RGBA, base_img_RGB = get_img(item["base_image"])
        layer_image_RGBA.append(self.to_tensor(base_img_RGBA))
        layer_image_RGB.append(self.to_tensor(base_img_RGB))
        layout.append(base_layout)

        layer_count = item["layer_count"]
        for i in range(layer_count):
            key = f"layer_{i:02d}"
            img_RGBA, img_RGB = get_img(item[key])
            
            w0, h0, w1, h1 = item[f"{key}_box"]

            canvas_RGBA = Image.new("RGBA", (W, H), (0, 0, 0, 0))
            canvas_RGB = Image.new("RGB", (W, H), (128, 128, 128))

            W_img, H_img = w1 - w0, h1 - h0
            if img_RGBA.size != (W_img, H_img):
                img_RGBA = img_RGBA.resize((W_img, H_img), Image.BILINEAR)
                img_RGB  = img_RGB.resize((W_img, H_img), Image.BILINEAR)

            canvas_RGBA.paste(img_RGBA, (w0, h0), img_RGBA)
            canvas_RGB.paste(img_RGB, (w0, h0))

            layer_image_RGBA.append(self.to_tensor(canvas_RGBA))
            layer_image_RGB.append(self.to_tensor(canvas_RGB))
            layout.append([w0, h0, w1, h1])

        return {
            "pixel_RGBA": layer_image_RGBA,
            "pixel_RGB": layer_image_RGB,
            "whole_img": whole_img_RGB,
            "caption": whole_cap,
            "height": H,
            "width": W,
            "layout": layout,
        }


def inference_layout_limited(config, max_samples: int = 5):
    """
    ä¿®æ”¹ç‰ˆçš„ inference_layoutï¼Œåªè™•ç†å‰ max_samples å€‹æ¨£æœ¬
    """
    import torch  # ç¢ºä¿ torch å·²å°å…¥
    
    if config.get('seed') is not None:
        infer_module.seed_everything(config['seed'])
    
    os.makedirs(config['save_dir'], exist_ok=True)
    os.makedirs(os.path.join(config['save_dir'], "merged"), exist_ok=True)
    os.makedirs(os.path.join(config['save_dir'], "merged_rgba"), exist_ok=True)

    # Load transparent VAEï¼ˆä½¿ç”¨åŸç‰ˆé‚è¼¯ï¼‰
    print("[INFO] Loading Transparent VAE...", flush=True)
    
    # æª¢æŸ¥ CUDA æ˜¯å¦å¯ç”¨
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    if not torch.cuda.is_available():
        print("[WARNING] CUDA is not available, using CPU (this will be very slow!)", flush=True)
    else:
        print(f"[INFO] Using device: {device}", flush=True)
    
    import argparse as argparse_module
    from models.transp_vae import AutoencoderKLTransformerTraining as CustomVAE
    
    vae_args = argparse_module.Namespace(
        max_layers=config.get('max_layers', 48),
        decoder_arch=config.get('decoder_arch', 'vit'),
        pos_embedding=config.get('pos_embedding', 'rope'),
        layer_embedding=config.get('layer_embedding', 'rope'),
        single_layer_decoder=config.get('single_layer_decoder', None)
    )
    transp_vae = CustomVAE(vae_args)
    transp_vae_path = config.get('transp_vae_path')
    
    # ä½¿ç”¨æ­£ç¢ºçš„è¨­å‚™åŠ è¼‰ï¼Œä¸¦æ˜ç¢ºæŒ‡å®š weights_only=Falseï¼ˆå› ç‚º checkpoint å¯èƒ½åŒ…å«éæ¨™æº–å°è±¡ï¼‰
    try:
        transp_vae_weights = torch.load(
            transp_vae_path, 
            map_location=device,
            weights_only=False  # CLD checkpoints å¯èƒ½åŒ…å« argparse.Namespace ç­‰éæ¨™æº–å°è±¡
        )
    except Exception as e:
        print(f"[ERROR] Failed to load transparent VAE weights: {e}", flush=True)
        print(f"[INFO] Trying to load on CPU first, then move to {device}...", flush=True)
        # å¦‚æœç›´æ¥åŠ è¼‰å¤±æ•—ï¼Œå…ˆåŠ è¼‰åˆ° CPUï¼Œç„¶å¾Œç§»å‹•åˆ°ç›®æ¨™è¨­å‚™
        transp_vae_weights = torch.load(
            transp_vae_path,
            map_location="cpu",
            weights_only=False
        )
        # ç§»å‹•æ¬Šé‡åˆ°ç›®æ¨™è¨­å‚™ï¼ˆå¦‚æœéœ€è¦çš„è©±ï¼‰
        if isinstance(transp_vae_weights, dict) and 'model' in transp_vae_weights:
            for k, v in transp_vae_weights['model'].items():
                if isinstance(v, torch.Tensor) and device.type == "cuda":
                    transp_vae_weights['model'][k] = v.to(device)
    
    missing_keys, unexpected_keys = transp_vae.load_state_dict(transp_vae_weights['model'], strict=False)
    if missing_keys:
        print(f"[WARNING] Missing keys: {missing_keys}", flush=True)
    if unexpected_keys:
        print(f"[WARNING] Unexpected keys: {unexpected_keys}", flush=True)
    transp_vae.eval()
    transp_vae = transp_vae.to(device)
    print("[INFO] Transparent VAE loaded successfully.", flush=True)

    # æ‡‰ç”¨ skip_fuse_lora patchï¼ˆå¦‚æœé…ç½®ä¸­å•Ÿç”¨ï¼‰
    apply_skip_fuse_lora_patch(config)

    # === é¡¯å¼åŠ è¼‰ T5 Encoder ä¸¦ä½¿ç”¨ NF4 é‡åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰ ===
    print("[INFO] å˜—è©¦é¡¯å¼åŠ è¼‰ T5 Encoder ä»¥å¯¦ç¾ NF4 é‡åŒ–...", flush=True)
    text_encoder_2 = None
    try:
        from transformers import T5EncoderModel
        import time

        # å¾ config ä¸­ç²å–æ¨¡å‹è·¯å¾‘
        model_path = config.get('model_path', 'black-forest-labs/FLUX.1-dev')

        load_kwargs = {
            'subfolder': 'text_encoder_2',
            'torch_dtype': torch.bfloat16,
        }

        # å¦‚æœæœ‰ NF4 é…ç½®ï¼Œæ·»åŠ é‡åŒ–åƒæ•¸
        if t5_quantization_config is not None:
            load_kwargs['quantization_config'] = t5_quantization_config
            # å°æ–¼é‡åŒ–çš„ T5ï¼Œè¨­ç½® device_map="cuda" ä»¥ç¢ºä¿æ­£ç¢ºåŠ è¼‰
            if torch.cuda.is_available():
                load_kwargs['device_map'] = "cuda"
            else:
                load_kwargs['device_map'] = "cpu"
            print("[INFO] ä½¿ç”¨ NF4 é‡åŒ–åŠ è¼‰ T5 Encoder", flush=True)
        else:
            print("[INFO] ä½¿ç”¨ bfloat16 åŠ è¼‰ T5 Encoderï¼ˆæœªå®‰è£ bitsandbytesï¼‰", flush=True)

        start_time = time.time()
        text_encoder_2 = T5EncoderModel.from_pretrained(model_path, **load_kwargs)
        elapsed = time.time() - start_time
        print(f"[INFO] T5 Encoder åŠ è¼‰å®Œæˆ in {elapsed:.2f}s", flush=True)

    except Exception as e:
        print(f"[WARNING] ç„¡æ³•é¡¯å¼åŠ è¼‰ T5 Encoder: {e}", flush=True)
        print("[INFO] å°‡ä½¿ç”¨ pipeline é»˜èªåŠ è¼‰é‚è¼¯", flush=True)
        text_encoder_2 = None

    # === ä¿®æ”¹ initialize_pipeline å‡½æ•¸ä»¥ä½¿ç”¨æˆ‘å€‘é åŠ è¼‰çš„ T5 ===
    original_initialize_pipeline = infer_module.initialize_pipeline

    def initialize_pipeline_with_t5(config):
        """Modified initialize_pipeline that uses our pre-loaded T5 model."""
        import time
        start_time = time.time()

        # å¦‚æœæˆ‘å€‘æˆåŠŸé åŠ è¼‰äº† T5ï¼Œå°‡å…¶å‚³éçµ¦ pipeline åˆå§‹åŒ–
        if text_encoder_2 is not None:
            print("[INFO] ä½¿ç”¨é åŠ è¼‰çš„ T5 Encoder åˆå§‹åŒ– pipeline", flush=True)
            # æˆ‘å€‘éœ€è¦æ””æˆªåŸå§‹çš„ initialize_pipeline ä¸¦æ³¨å…¥æˆ‘å€‘çš„ T5
            # ç”±æ–¼ CLD çš„ initialize_pipeline å¯èƒ½ä¸æ”¯æŒç›´æ¥å‚³å…¥ text_encoder_2
            # æˆ‘å€‘éœ€è¦ patch å®ƒæˆ–è€…å‰µå»ºä¸€å€‹ wrapper

            # å‰µå»ºä¸€å€‹å‡çš„ configï¼Œå‘Šè¨´ initialize_pipeline ä¸è¦åŠ è¼‰ text_encoder_2
            modified_config = config.copy()
            if 'text_encoder_2_path' not in modified_config:
                modified_config['text_encoder_2_path'] = None  # æˆ–è€…è¨­ç½®ç‚ºå‡è·¯å¾‘

            # èª¿ç”¨åŸå§‹çš„ initialize_pipeline
            pipeline = original_initialize_pipeline(modified_config)

            # æ‰‹å‹•æ›¿æ› pipeline çš„ text_encoder_2
            if hasattr(pipeline, 'text_encoder_2'):
                pipeline.text_encoder_2 = text_encoder_2
                print("[INFO] å·²å°‡é åŠ è¼‰çš„ T5 Encoder æ³¨å…¥åˆ° pipeline ä¸­", flush=True)
            else:
                print("[WARNING] Pipeline æ²’æœ‰ text_encoder_2 å±¬æ€§ï¼Œç„¡æ³•æ³¨å…¥ T5", flush=True)

            elapsed = time.time() - start_time
            print(f"[INFO] Pipeline åˆå§‹åŒ–å®Œæˆ (with T5 injection) in {elapsed:.2f}s", flush=True)
            return pipeline
        else:
            # å¦‚æœæ²’æœ‰é åŠ è¼‰ T5ï¼Œä½¿ç”¨åŸå§‹é‚è¼¯
            print("[INFO] ä½¿ç”¨åŸå§‹é‚è¼¯åˆå§‹åŒ– pipeline", flush=True)
            pipeline = original_initialize_pipeline(config)
            elapsed = time.time() - start_time
            print(f"[INFO] Pipeline åˆå§‹åŒ–å®Œæˆ in {elapsed:.2f}s", flush=True)
            return pipeline

    # æ›¿æ› infer_module çš„ initialize_pipeline å‡½æ•¸
    infer_module.initialize_pipeline = initialize_pipeline_with_t5

    # åˆå§‹åŒ– pipelineï¼ˆç¾åœ¨æœƒä½¿ç”¨æˆ‘å€‘çš„ä¿®æ”¹ç‰ˆï¼‰
    pipeline = infer_module.initialize_pipeline(config)
    
    # Check if LoRA adapters are properly loaded and activated
    print("\n[DEBUG] Checking LoRA adapter status...", flush=True)
    try:
        # Check if pipeline has get_active_adapters method (diffusers standard)
        if hasattr(pipeline, 'get_active_adapters'):
            active_adapters = pipeline.get_active_adapters()
            print(f"  âœ… Active adapters: {active_adapters}", flush=True)
            if active_adapters and 'layer' in active_adapters:
                print(f"  âœ… Adapter 'layer' is active!", flush=True)
            else:
                print(f"  âš ï¸  Adapter 'layer' is NOT in active adapters!", flush=True)
        else:
            print("  âš ï¸  Pipeline does not have get_active_adapters() method", flush=True)
        
        # Check adapter names
        if hasattr(pipeline, 'get_adapter_names'):
            adapter_names = pipeline.get_adapter_names()
            print(f"  Available adapter names: {adapter_names}", flush=True)
        else:
            print("  âš ï¸  Pipeline does not have get_adapter_names() method", flush=True)
        
        # Check if transformer has LoRA layers (for PEFT-based LoRA)
        if hasattr(pipeline, 'transformer'):
            transformer = pipeline.transformer
            print(f"\n[DEBUG] Checking transformer for LoRA layers...", flush=True)
            
            # Count LoRA layers in transformer
            lora_layers_count = 0
            lora_layer_names = []
            for name, module in transformer.named_modules():
                # Check for common LoRA layer patterns
                if 'lora' in name.lower() or hasattr(module, 'lora_A') or hasattr(module, 'lora_B'):
                    lora_layers_count += 1
                    lora_layer_names.append(name)
            
            if lora_layers_count > 0:
                print(f"  âœ… Found {lora_layers_count} LoRA layers in transformer", flush=True)
                if lora_layers_count <= 10:
                    print(f"  LoRA layer names: {lora_layer_names}", flush=True)
                else:
                    print(f"  First 10 LoRA layer names: {lora_layer_names[:10]}...", flush=True)
            else:
                print(f"  âš ï¸  No LoRA layers found in transformer!", flush=True)
                print(f"  âš ï¸  This might indicate LoRA weights were not loaded correctly", flush=True)
        
        # If skip_fuse_lora=True, adapter should already be active via PEFT mechanism
        skip_fuse_lora = config.get('skip_fuse_lora', False)
        if skip_fuse_lora:
            print(f"\n[DEBUG] skip_fuse_lora=True: LoRA should work via PEFT mechanism", flush=True)
            if hasattr(pipeline, 'get_active_adapters'):
                active = pipeline.get_active_adapters()
                if active and 'layer' in active:
                    print(f"  âœ… Adapter is active, LoRA should be working via PEFT", flush=True)
                else:
                    print(f"  âš ï¸  Adapter is NOT active! This might be the problem!", flush=True)
                    # Try to set adapter explicitly (diffusers standard method)
                    if hasattr(pipeline, 'set_adapters'):
                        try:
                            # Try different ways to set adapter
                            import inspect
                            sig = inspect.signature(pipeline.set_adapters)
                            print(f"  set_adapters signature: {sig}", flush=True)
                            # The adapter was loaded with adapter_name="layer" in initialize_pipeline
                            pipeline.set_adapters(["layer"], adapter_weights=[1.0])
                            print("  âœ… Explicitly enabled adapter 'layer' with weight 1.0", flush=True)
                        except Exception as e:
                            print(f"  âš ï¸  Failed to set adapters: {e}", flush=True)
                            import traceback
                            traceback.print_exc()
                    else:
                        print("  âš ï¸  Pipeline does not have set_adapters() method", flush=True)
    except Exception as e:
        print(f"  âš ï¸  Error checking adapter status: {e}", flush=True)
        import traceback
        traceback.print_exc()
    print("", flush=True)

    # å‰µå»º datasetï¼ˆä½¿ç”¨ä¿®æ”¹ç‰ˆï¼Œåœ¨åˆå§‹åŒ–æ™‚å°±é™åˆ¶æ¨£æœ¬æ•¸é‡ï¼‰
    print(f"[INFO] å‰µå»º datasetï¼ˆå°‡é™åˆ¶ç‚ºå‰ {max_samples} å€‹æ¨£æœ¬ï¼‰...", flush=True)
    
    # ä½¿ç”¨ä¿®æ”¹ç‰ˆçš„ datasetï¼Œåœ¨ split ä¹‹å¾Œç«‹å³é™åˆ¶æ¨£æœ¬æ•¸é‡
    dataset = LimitedLayoutTrainDataset(config['data_dir'], split="test", max_samples=max_samples)
    
    loader = infer_module.DataLoader(
        dataset, 
        batch_size=1, 
        shuffle=False, 
        num_workers=0, 
        collate_fn=infer_module.collate_fn
    )

    generator = torch.Generator(device=device).manual_seed(config.get('seed', 42))
    import gc  # ç”¨æ–¼å¼·åˆ¶åƒåœ¾å›æ”¶ï¼Œå¹«åŠ©é‡‹æ”¾ VRAM
    
    idx = 0
    actual_samples = len(dataset)  # ç²å–å¯¦éš›çš„æ¨£æœ¬æ•¸é‡
    for batch in loader:
        print(f"\n{'='*60}")
        print(f"Processing case {idx} (æ¨£æœ¬ {idx+1}/{actual_samples})")
        print(f"{'='*60}", flush=True)

        height = int(batch["height"][0])
        width = int(batch["width"][0])
        adapter_img = batch["whole_img"][0]
        caption = batch["caption"][0]
        layer_boxes = infer_module.get_input_box(batch["layout"][0]) 

        # Debug: é¡¯ç¤º layout è³‡è¨Š
        print(f"[DEBUG] Image size: {width}x{height}", flush=True)
        print(f"[DEBUG] Layout boxes count: {len(layer_boxes)}", flush=True)
        if len(caption) > 100:
            print(f"[DEBUG] Caption: {caption[:100]}...", flush=True)
        else:
            print(f"[DEBUG] Caption: {caption}", flush=True)

        # åœ¨æ¯æ¬¡æ¨ç†å‰å˜—è©¦æ¸…ä¸€æ¬¡ CUDA cacheï¼ˆé¿å…å‰ä¸€å¼µåœ–æ®˜ç•™ä½”ç”¨ VRAMï¼‰
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # Generate layers using pipelineï¼ˆä½¿ç”¨åŸç‰ˆé‚è¼¯ï¼‰
        with torch.no_grad():
            x_hat, image, latents = pipeline(
                prompt=caption,
                adapter_image=adapter_img,
                adapter_conditioning_scale=0.9,
                validation_box=layer_boxes,
                generator=generator,
                height=height,
                width=width,
                guidance_scale=config.get('cfg', 4.0),
                num_layers=len(layer_boxes),
                sdxl_vae=transp_vae,  # Use transparent VAE
            )

        # Adjust x_hat range from [-1, 1] to [0, 1]
        x_hat = (x_hat + 1) / 2

        # Remove batch dimensionï¼Œä¸¦ç«‹åˆ»æ¬åˆ° CPUï¼Œæ¸›å°‘ GPU VRAM ä½”ç”¨
        x_hat = x_hat.squeeze(0).permute(1, 0, 2, 3).cpu().to(torch.float32)
        
        # åŒæ¨£æŠŠ image æ¬åˆ° CPU
        if isinstance(image, torch.Tensor):
            image = image.cpu()
        elif isinstance(image, (list, tuple)):
            image = [img.cpu() if isinstance(img, torch.Tensor) else img for img in image]
        
        # latents ä¹‹å¾Œä¸å†ç”¨ï¼Œç›´æ¥åˆªæ‰ä¸¦æ¸…ç† cache
        del latents
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        this_index = f"case_{idx}"
        case_dir = os.path.join(config['save_dir'], this_index)
        os.makedirs(case_dir, exist_ok=True)
        
        # Save whole image_RGBA (X_hat[0]) and background_RGBA (X_hat[1])
        whole_image_layer = (x_hat[0].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)
        whole_image_rgba_image = Image.fromarray(whole_image_layer, "RGBA")
        whole_image_rgba_image.save(os.path.join(case_dir, "whole_image_rgba.png"))

        adapter_img.save(os.path.join(case_dir, "origin.png"))

        background_layer = (x_hat[1].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)
        background_rgba_image = Image.fromarray(background_layer, "RGBA")
        background_rgba_image.save(os.path.join(case_dir, "background_rgba.png"))

        x_hat = x_hat[2:]
        merged_image = image[1]
        image = image[2:]

        # Save transparent VAE decoded resultsï¼ˆæ·»åŠ  alpha channel è¨ºæ–·ï¼‰
        print(f"[DEBUG] Saving {x_hat.shape[0]} foreground layers...", flush=True)
        for layer_idx in range(x_hat.shape[0]):
            layer = x_hat[layer_idx]
            rgba_layer = (layer.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)
            
            # Debug: æª¢æŸ¥ alpha channel
            alpha_channel = rgba_layer[:, :, 3]
            alpha_min, alpha_max = int(alpha_channel.min()), int(alpha_channel.max())
            alpha_mean = float(alpha_channel.mean())
            transparent_pixels = int((alpha_channel == 0).sum())
            total_pixels = alpha_channel.size
            transparent_ratio = transparent_pixels / total_pixels * 100
            
            # ç²å–å°æ‡‰çš„ box
            if layer_idx < len(layer_boxes) - 2:
                corresponding_box = layer_boxes[layer_idx + 2]
                x1, y1, x2, y2 = corresponding_box
                box_area = (x2 - x1) * (y2 - y1)
                print(f"  Layer {layer_idx}: box={corresponding_box}, box_area={box_area}, "
                      f"alpha_range=[{alpha_min}, {alpha_max}], alpha_mean={alpha_mean:.1f}, "
                      f"transparent={transparent_ratio:.1f}%", flush=True)
            else:
                print(f"  Layer {layer_idx}: alpha_range=[{alpha_min}, {alpha_max}], "
                      f"alpha_mean={alpha_mean:.1f}, transparent={transparent_ratio:.1f}%", flush=True)
            
            rgba_image = Image.fromarray(rgba_layer, "RGBA")
            rgba_image.save(os.path.join(case_dir, f"layer_{layer_idx}_rgba.png"))

        # Composite background and foreground layers
        for layer_idx in range(x_hat.shape[0]):
            rgba_layer = (x_hat[layer_idx].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)
            layer_image = Image.fromarray(rgba_layer, "RGBA")
            merged_image = Image.alpha_composite(merged_image.convert('RGBA'), layer_image)
        
        # Save final composite images
        merged_image.convert('RGB').save(os.path.join(config['save_dir'], "merged", f"{this_index}.png"))
        merged_image.convert('RGB').save(os.path.join(case_dir, f"{this_index}.png"))
        # Save final composite RGBA image
        merged_image.save(os.path.join(config['save_dir'], "merged_rgba", f"{this_index}.png"))

        print(f"âœ… Saved case {idx} to {case_dir}")
        idx += 1

        # === æ¯å¼µåœ–ç‰‡ä¹‹å¾Œåšä¸€æ¬¡å¼·åˆ¶æ¸…ç†ï¼Œç›¡é‡é‡‹æ”¾ VRAM ===
        try:
            # åˆªæ‰æœ¬è¼ªå¤§ tensor è®Šæ•¸
            del x_hat
            del image
            del merged_image
        except NameError:
            pass

        # Python åƒåœ¾å›æ”¶
        gc.collect()

        # CUDA è¨˜æ†¶é«”å›æ”¶
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            try:
                torch.cuda.synchronize()
            except Exception:
                pass

    del pipeline
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    print(f"\n{'='*60}")
    print(f"âœ… æ¸¬è©¦å®Œæˆï¼è™•ç†äº† {idx} å€‹æ¨£æœ¬")
    print(f"   è¼¸å‡ºç›®éŒ„ï¼š{config['save_dir']}")
    print(f"{'='*60}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description="ä½¿ç”¨åŸç‰ˆ CLD infer.py æ¸¬è©¦ï¼Œä½†åªè™•ç†å°‘é‡æ¨£æœ¬"
    )
    parser.add_argument(
        "--config_path", "-c", 
        type=str, 
        required=True, 
        help="Path to the YAML configuration file."
    )
    parser.add_argument(
        "--max_samples", "-n",
        type=int,
        default=5,
        help="æœ€å¤šè™•ç†çš„æ¨£æœ¬æ•¸é‡ï¼ˆé è¨­ï¼š5ï¼‰"
    )
    args = parser.parse_args()

    # å°å…¥å¿…è¦çš„æ¨¡çµ„ï¼ˆtorch å·²ç¶“åœ¨é ‚éƒ¨å°å…¥ï¼‰
    import numpy as np
    from PIL import Image

    # CUDA æª¢æŸ¥å·²ç¶“åœ¨é ‚éƒ¨å®Œæˆï¼Œé€™è£¡ç›´æ¥åŠ è¼‰é…ç½®
    config = infer_module.load_config(args.config_path)
    
    # æ‡‰ç”¨ skip_fuse_lora patchï¼ˆå¦‚æœé…ç½®ä¸­å•Ÿç”¨ï¼‰
    # é€™éœ€è¦åœ¨åŠ è¼‰ config ä¹‹å¾Œï¼Œä½†åœ¨ initialize_pipeline ä¹‹å‰
    apply_skip_fuse_lora_patch(config)
    
    print(f"\n{'='*60}")
    print("CLD æ¸¬è©¦è…³æœ¬ï¼ˆé™åˆ¶æ¨£æœ¬æ•¸é‡ï¼‰")
    print(f"{'='*60}")
    print(f"é…ç½®æª”æ¡ˆï¼š{args.config_path}")
    print(f"æœ€å¤šè™•ç†æ¨£æœ¬æ•¸ï¼š{args.max_samples}")
    print(f"è¼¸å‡ºç›®éŒ„ï¼š{config['save_dir']}")
    if config.get('skip_fuse_lora', False):
        print(f"è¨˜æ†¶é«”å„ªåŒ–ï¼šskip_fuse_lora=True (è·³é fuse_lora ä»¥ç¯€çœè¨˜æ†¶é«”)")
    print(f"{'='*60}\n")
    print("âš ï¸  æ³¨æ„ï¼šé¦–æ¬¡é‹è¡Œæ™‚æœƒä¸‹è¼‰ PrismLayersPro dataset çš„ metadata")
    print("   ä½†åªæœƒè™•ç†å‰ {} å€‹æ¨£æœ¬ï¼Œä¸æœƒä¸‹è¼‰æ•´å€‹ 100GB+ dataset\n".format(args.max_samples))

    try:
        inference_layout_limited(config, max_samples=args.max_samples)
        raise SystemExit(0)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ¶ä¸­æ–·")
        raise SystemExit(130)
    except SystemExit:
        raise
    except Exception as e:
        print(f"\n\nâŒ éŒ¯èª¤ï¼š{e}", flush=True)
        import traceback
        traceback.print_exc()
        raise SystemExit(1)


