#!/usr/bin/env python3
"""
ç°¡å–®çš„ wrapperï¼šä½¿ç”¨åŸç‰ˆ infer.pyï¼Œä½†åªè™•ç†å‰ N å€‹æ¨£æœ¬

é€™å€‹è…³æœ¬æœƒï¼š
1. ç›´æ¥èª¿ç”¨åŸç‰ˆ infer.py çš„å‡½æ•¸
2. åœ¨ DataLoader å±¤é¢é™åˆ¶æ¨£æœ¬æ•¸é‡
3. é¿å…ä¸‹è¼‰æ•´å€‹ 100GB+ datasetï¼ˆé›–ç„¶é¦–æ¬¡ä»æœƒä¸‹è¼‰ metadataï¼‰

ä½¿ç”¨æ–¹å¼ï¼š
    cd /home/hpc/ce505203/finals_repo/third_party/cld/infer
    python ../../../scripts/test_original_cld_limited.py --config_path <config.yaml> --max_samples 5
"""

import sys
import os
from pathlib import Path
import argparse

# å…ˆæª¢æŸ¥ CUDA å¯ç”¨æ€§ï¼ˆåœ¨å°å…¥ infer.py ä¹‹å‰ï¼‰
# å› ç‚º infer.py æœƒè¨­ç½® CUDA_VISIBLE_DEVICES = "1"ï¼Œæˆ‘å€‘éœ€è¦å…ˆè™•ç†
import torch

# è¨­å®šè·¯å¾‘
script_path = Path(__file__).resolve()
repo_root = script_path.parent.parent
cld_root = repo_root / "third_party" / "cld"
cld_infer_dir = cld_root / "infer"

sys.path.insert(0, str(cld_root))
os.chdir(str(cld_root))

# æª¢æŸ¥ CUDA å¯ç”¨æ€§
cuda_available = torch.cuda.is_available()
if cuda_available:
    device_count = torch.cuda.device_count()
    print(f"âœ… CUDA å¯ç”¨ï¼š{torch.cuda.get_device_name(0)}")
    print(f"   CUDA ç‰ˆæœ¬ï¼š{torch.version.cuda}")
    print(f"   å¯ç”¨ GPU æ•¸é‡ï¼š{device_count}\n")
else:
    print("\n" + "="*60)
    print("âš ï¸  è­¦å‘Šï¼šCUDA ä¸å¯ç”¨ï¼")
    print("="*60)
    print("CLD inference éœ€è¦ GPU æ‰èƒ½é‹è¡Œã€‚")
    print("å¦‚æœæ²’æœ‰ GPUï¼Œæ¨ç†æœƒéå¸¸æ…¢ï¼ˆå¯èƒ½éœ€è¦æ•¸å°æ™‚ï¼‰ã€‚")
    print("\nå»ºè­°ï¼š")
    print("1. ç¢ºä¿ GPU å¯ç”¨ï¼šnvidia-smi")
    print("2. ç¢ºä¿ CUDA_VISIBLE_DEVICES è¨­ç½®æ­£ç¢º")
    print("3. ç¢ºä¿ PyTorch å®‰è£äº† CUDA æ”¯æŒ")
    print("="*60 + "\n")
    
    response = input("æ˜¯å¦ç¹¼çºŒä½¿ç”¨ CPUï¼Ÿï¼ˆy/Nï¼‰: ")
    if response.lower() != 'y':
        print("å·²å–æ¶ˆã€‚")
        raise SystemExit(1)

# å°å…¥åŸç‰ˆ infer.py
# æ³¨æ„ï¼šinfer.py æœƒè¨­ç½® CUDA_VISIBLE_DEVICES = "1"
# å¦‚æœç³»çµ±åªæœ‰ GPU 0ï¼Œé€™æœƒå°è‡´å•é¡Œï¼Œæ‰€ä»¥æˆ‘å€‘éœ€è¦ä¿®æ”¹ä»£ç¢¼
import importlib.util
infer_py_path = cld_infer_dir / "infer.py"

# è®€å– infer.py çš„ä»£ç¢¼
with open(infer_py_path, 'r', encoding='utf-8') as f:
    infer_code = f.read()

# å¦‚æœåªæœ‰ä¸€å€‹ GPUï¼Œä¿®æ”¹ CUDA_VISIBLE_DEVICES è¨­ç½®
if cuda_available and device_count == 1:
    # å°‡ CUDA_VISIBLE_DEVICES = "1" æ”¹ç‚º "0" æˆ–ç§»é™¤
    if 'os.environ["CUDA_VISIBLE_DEVICES"] = "1"' in infer_code:
        infer_code = infer_code.replace(
            'os.environ["CUDA_VISIBLE_DEVICES"] = "1"',
            'os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # ä¿®æ”¹ç‚ºä½¿ç”¨ GPU 0'
        )
        print("[INFO] æª¢æ¸¬åˆ°åªæœ‰ä¸€å€‹ GPUï¼Œå·²ä¿®æ”¹ infer.py ä¸­çš„ CUDA_VISIBLE_DEVICES=1 â†’ 0\n")

# å‰µå»ºæ¨¡çµ„ä¸¦åŸ·è¡Œä¿®æ”¹å¾Œçš„ä»£ç¢¼
spec = importlib.util.spec_from_file_location("infer_module", str(infer_py_path))
infer_module = importlib.util.module_from_spec(spec)
# åŸ·è¡Œä¿®æ”¹å¾Œçš„ä»£ç¢¼
exec(compile(infer_code, str(infer_py_path), 'exec'), infer_module.__dict__)

from torch.utils.data import Subset, Dataset

# Optimize LoRA loading: Monkey patch CustomFluxPipeline.lora_state_dict AFTER loading infer_module
# This ensures LoRA weights are loaded directly to GPU using safetensors for faster loading
print("[DEBUG] Starting LoRA optimization setup...", flush=True)
CustomFluxPipeline = None  # Will be set in try block
try:
    import time
    
    # Import CustomFluxPipeline from the loaded module
    # This import may be slow if models.pipeline is large
    print("[DEBUG] Importing CustomFluxPipeline (this may take a moment)...", flush=True)
    import_start = time.time()
    from models.pipeline import CustomFluxPipeline
    import_elapsed = time.time() - import_start
    print(f"[DEBUG] CustomFluxPipeline imported in {import_elapsed:.2f}s", flush=True)
    
    # Store original lora_state_dict method
    print("[DEBUG] Checking for lora_state_dict method...", flush=True)
    if hasattr(CustomFluxPipeline, 'lora_state_dict'):
        print("[DEBUG] Found lora_state_dict, creating optimized version...", flush=True)
        original_lora_state_dict = CustomFluxPipeline.lora_state_dict
        
        @staticmethod
        def optimized_lora_state_dict(lora_path, *args, **kwargs):
            """
            Optimized LoRA loading that:
            1. Ensures safetensors format is used
            2. Loads directly to GPU (cuda)
            3. Provides progress indication
            
            Returns:
                - If called by load_lora_into_transformer: state_dict only
                - If called by pipeline.load_lora_weights(): (state_dict, network_alphas) tuple
            """
            import safetensors.torch
            
            lora_path_obj = Path(lora_path)
            
            # Check if it's a directory or file path
            if lora_path_obj.is_dir():
                # Look for safetensors file in directory
                safetensors_file = lora_path_obj / "pytorch_lora_weights.safetensors"
                if not safetensors_file.exists():
                    # Fallback: try to find any safetensors file
                    safetensors_files = list(lora_path_obj.glob("*.safetensors"))
                    if safetensors_files:
                        safetensors_file = safetensors_files[0]
                    else:
                        print(f"âš ï¸  Warning: No safetensors file found in {lora_path}, falling back to original method")
                        return original_lora_state_dict(lora_path, *args, **kwargs)
                lora_file = safetensors_file
            elif lora_path_obj.suffix == ".safetensors":
                lora_file = lora_path_obj
            else:
                # Not safetensors format, try to find safetensors version
                safetensors_file = lora_path_obj.parent / f"{lora_path_obj.stem}.safetensors"
                if safetensors_file.exists():
                    lora_file = safetensors_file
                    print(f"   ğŸ“¦ Found safetensors version: {lora_file}")
                else:
                    print(f"âš ï¸  Warning: LoRA file is not safetensors format: {lora_path}")
                    print(f"   Expected safetensors file: {safetensors_file}")
                    print(f"   Falling back to original method (may be slower)")
                    return original_lora_state_dict(lora_path, *args, **kwargs)
            
            if not lora_file.exists():
                print(f"âš ï¸  Warning: LoRA file not found: {lora_file}, falling back to original method")
                return original_lora_state_dict(lora_path, *args, **kwargs)
            
            print(f"   ğŸ“¦ Loading LoRA weights from: {lora_file}", flush=True)
            print(f"   âœ… Using safetensors format (faster loading)", flush=True)
            
            start_time = time.time()
            
            # Determine device - prefer GPU if available
            device = "cuda" if torch.cuda.is_available() else "cpu"
            if device == "cpu":
                print(f"   âš ï¸  Warning: CUDA not available, loading to CPU (will be slower)", flush=True)
            
            try:
                # Load safetensors - try direct GPU loading first, fallback to CPU then move to GPU
                # safetensors.torch.load_file returns a dict of tensors
                load_start = time.time()
                try:
                    # Try loading directly to GPU if device parameter is supported
                    state_dict = safetensors.torch.load_file(str(lora_file), device=device)
                    load_elapsed = time.time() - load_start
                    loaded_to_gpu = (device == "cuda")
                    print(f"   â±ï¸  Safetensors I/O: {load_elapsed:.2f}s", flush=True)
                except TypeError:
                    # device parameter not supported, load to CPU then move to GPU
                    state_dict = safetensors.torch.load_file(str(lora_file))
                    load_elapsed = time.time() - load_start
                    print(f"   â±ï¸  Safetensors I/O (CPU): {load_elapsed:.2f}s", flush=True)
                    
                    if device == "cuda" and torch.cuda.is_available():
                        # Move all tensors to GPU efficiently
                        # Batch move for better performance (avoids multiple small transfers)
                        move_start = time.time()
                        # Collect all tensors first, then move them in batch
                        tensor_keys = [k for k, v in state_dict.items() if isinstance(v, torch.Tensor)]
                        if tensor_keys:
                            # Use non_blocking=True for async transfer, but we'll sync at the end
                            for k in tensor_keys:
                                state_dict[k] = state_dict[k].cuda(non_blocking=True)
                            # Synchronize to ensure all transfers complete before returning
                            torch.cuda.synchronize()
                        move_elapsed = time.time() - move_start
                        print(f"   â±ï¸  CPU->GPU transfer ({len(tensor_keys)} tensors): {move_elapsed:.2f}s", flush=True)
                        loaded_to_gpu = True
                    else:
                        loaded_to_gpu = False
                
                elapsed = time.time() - start_time
                num_keys = len(state_dict)
                file_size_mb = lora_file.stat().st_size / (1024 * 1024)
                
                print(f"   âœ… LoRA loaded ({num_keys} keys, {file_size_mb:.2f} MB) in {elapsed:.2f}s total", flush=True)
                if loaded_to_gpu:
                    print(f"   ğŸš€ Loaded directly to GPU (optimized path)", flush=True)
                
                # Always return tuple (state_dict, network_alphas) as expected by load_lora_weights
                # load_lora_into_transformer wrapper will handle unpacking
                network_alphas = None
                return state_dict, network_alphas
                
            except Exception as e:
                print(f"   âš ï¸  Error loading safetensors: {e}", flush=True)
                print(f"   Falling back to original method", flush=True)
                return original_lora_state_dict(lora_path, *args, **kwargs)
        
        # Apply monkey patch
        print("[DEBUG] About to apply monkey patch to CustomFluxPipeline.lora_state_dict...", flush=True)
        CustomFluxPipeline.lora_state_dict = optimized_lora_state_dict
        print("[DEBUG] Monkey patch applied successfully", flush=True)
        print("âœ… Applied LoRA loading optimization: safetensors + direct GPU loading", flush=True)
        print("[DEBUG] Finished LoRA optimization setup", flush=True)
        
        # Also patch load_lora_into_transformer to handle tuple return from optimized_lora_state_dict
        if hasattr(CustomFluxPipeline, 'load_lora_into_transformer'):
            original_load_lora = CustomFluxPipeline.load_lora_into_transformer
            
            @staticmethod
            def timed_load_lora_into_transformer(lora_state_dict, *args, **kwargs):
                print("[DEBUG] load_lora_into_transformer: Starting...", flush=True)
                start = time.time()
                
                # Handle case where lora_state_dict might be a tuple (from optimized_lora_state_dict)
                # load_lora_into_transformer expects just the state_dict, not the tuple
                if isinstance(lora_state_dict, tuple):
                    lora_state_dict, _ = lora_state_dict  # Unpack tuple, ignore network_alphas
                
                result = original_load_lora(lora_state_dict, *args, **kwargs)
                elapsed = time.time() - start
                print(f"[DEBUG] load_lora_into_transformer: Completed in {elapsed:.2f}s", flush=True)
                return result
            
            CustomFluxPipeline.load_lora_into_transformer = timed_load_lora_into_transformer
        
except ImportError as e:
    print(f"âš ï¸  Warning: Could not apply LoRA loading optimization: {e}")
    print("   LoRA loading will use default method (may be slower)")
except Exception as e:
    print(f"âš ï¸  Warning: Error applying LoRA loading optimization: {e}")
    print("   LoRA loading will use default method (may be slower)")

# Patch fuse_lora and unload_lora to add timing (similar to infer_dlcv.py)
try:
    from models.mmdit import CustomFluxTransformer2DModel
    
    if hasattr(CustomFluxTransformer2DModel, 'fuse_lora'):
        original_fuse_lora = CustomFluxTransformer2DModel.fuse_lora
        
        def timed_fuse_lora(self, *args, **kwargs):
            import time
            print("[DEBUG] fuse_lora: Starting...", flush=True)
            start = time.time()
            result = original_fuse_lora(self, *args, **kwargs)
            elapsed = time.time() - start
            print(f"[DEBUG] fuse_lora: Completed in {elapsed:.2f}s", flush=True)
            return result
        
        CustomFluxTransformer2DModel.fuse_lora = timed_fuse_lora
    
    if hasattr(CustomFluxTransformer2DModel, 'unload_lora'):
        original_unload_lora = CustomFluxTransformer2DModel.unload_lora
        
        def timed_unload_lora(self, *args, **kwargs):
            import time
            print("[DEBUG] unload_lora: Starting...", flush=True)
            start = time.time()
            result = original_unload_lora(self, *args, **kwargs)
            elapsed = time.time() - start
            print(f"[DEBUG] unload_lora: Completed in {elapsed:.2f}s", flush=True)
            return result
        
        CustomFluxTransformer2DModel.unload_lora = timed_unload_lora
except ImportError:
    print("[DEBUG] Could not patch fuse_lora/unload_lora (models.mmdit not available)", flush=True)
except Exception as e:
    print(f"[DEBUG] Warning: Error patching fuse_lora/unload_lora: {e}", flush=True)


def apply_skip_fuse_lora_patch(config):
    """
    å¦‚æœ config ä¸­è¨­ç½®äº† skip_fuse_lora=Trueï¼Œå‰‡ patch fuse_lora å’Œ unload_lora ç‚º no-op
    é€™æ¨£å¯ä»¥ç¯€çœè¨˜æ†¶é«”ï¼ŒLoRA æœƒé€šé PEFT æ©Ÿåˆ¶å·¥ä½œï¼ˆæ€§èƒ½æå¤± <5%ï¼‰
    """
    skip_fuse_lora = config.get('skip_fuse_lora', False)
    
    if not skip_fuse_lora:
        return
    
    print("âš ï¸  skip_fuse_lora=True: Will skip fuse_lora() to save memory", flush=True)
    print("   LoRA will work via PEFT mechanism (slight performance loss <5%, but saves memory)", flush=True)
    
    # Monkey patch fuse_lora and unload_lora to be no-ops
    if CustomFluxPipeline is not None:
        try:
            from models.mmdit import CustomFluxTransformer2DModel
            from models.multiLayer_adapter import MultiLayerAdapter
            
            # Patch fuse_lora to be a no-op
            if hasattr(CustomFluxTransformer2DModel, 'fuse_lora'):
                def noop_fuse_lora(self, *args, **kwargs):
                    print("[INFO] Skipping fuse_lora() to save memory (LoRA will work via PEFT)", flush=True)
                    return None
                CustomFluxTransformer2DModel.fuse_lora = noop_fuse_lora
            
            if hasattr(MultiLayerAdapter, 'fuse_lora'):
                def noop_fuse_lora_adapter(self, *args, **kwargs):
                    print("[INFO] Skipping MultiLayerAdapter.fuse_lora() to save memory", flush=True)
                    return None
                MultiLayerAdapter.fuse_lora = noop_fuse_lora_adapter
            
            # Patch unload_lora to be a no-op (since we didn't fuse, we shouldn't unload)
            if hasattr(CustomFluxTransformer2DModel, 'unload_lora'):
                def noop_unload_lora(self, *args, **kwargs):
                    print("[INFO] Skipping unload_lora() (LoRA weights kept for PEFT inference)", flush=True)
                    return None
                CustomFluxTransformer2DModel.unload_lora = noop_unload_lora
            
            if hasattr(MultiLayerAdapter, 'unload_lora'):
                def noop_unload_lora_adapter(self, *args, **kwargs):
                    print("[INFO] Skipping MultiLayerAdapter.unload_lora()", flush=True)
                    return None
                MultiLayerAdapter.unload_lora = noop_unload_lora_adapter
            
            print("âœ… Patched fuse_lora/unload_lora to skip (memory optimization)", flush=True)
        except ImportError:
            print("âš ï¸  Warning: Could not patch fuse_lora/unload_lora (models not available)", flush=True)
        except Exception as e:
            print(f"âš ï¸  Warning: Error patching fuse_lora/unload_lora: {e}", flush=True)


class LimitedLayoutTrainDataset(Dataset):
    """
    ä¿®æ”¹ç‰ˆçš„ LayoutTrainDatasetï¼Œåœ¨åˆå§‹åŒ–æ™‚å°±é™åˆ¶æ¨£æœ¬æ•¸é‡
    é€™æ¨£å¯ä»¥é¿å…è™•ç†æ•´å€‹ datasetï¼ˆé›–ç„¶ metadata é‚„æ˜¯æœƒä¸‹è¼‰ï¼‰
    """
    def __init__(self, data_dir, split="test", max_samples=None):
        from datasets import load_dataset, concatenate_datasets
        from collections import defaultdict
        import numpy as np
        import torchvision.transforms as T
        from PIL import Image
        
        print(f"[INFO] åŠ è¼‰ PrismLayersPro datasetï¼ˆsplit={split}ï¼‰...", flush=True)
        print(f"[INFO] æ³¨æ„ï¼šHuggingFace æœƒä¸‹è¼‰ metadata æ–‡ä»¶ï¼ˆ~400-500MBï¼‰ï¼Œé€™æ˜¯æ­£å¸¸çš„", flush=True)
        print(f"[INFO] åœ–ç‰‡æœƒæŒ‰éœ€ä¸‹è¼‰ï¼Œåªæœƒä¸‹è¼‰å¯¦éš›è¨ªå•çš„æ¨£æœ¬", flush=True)
        
        full_dataset = load_dataset(
            "artplus/PrismLayersPro",
            cache_dir=data_dir,
        )
        full_dataset = concatenate_datasets(list(full_dataset.values()))

        if "style_category" not in full_dataset.column_names:
            raise ValueError("Dataset must contain a 'style_category' field to split by class.")

        categories = np.array(full_dataset["style_category"])
        category_to_indices = defaultdict(list)
        for i, cat in enumerate(categories):
            category_to_indices[cat].append(i)

        subsets = []
        for cat, indices in category_to_indices.items():
            total_len = len(indices)
            idx_90 = int(total_len * 0.9)
            idx_95 = int(total_len * 0.95)

            if split == "train":
                selected_idx = indices[:idx_90]
            elif split == "test":
                selected_idx = indices[idx_90:idx_95]
            elif split == "val":
                selected_idx = indices[idx_95:]
            else:
                raise ValueError("split must be 'train', 'val', or 'test'")

            subsets.append(full_dataset.select(selected_idx))

        # åˆä½µæ‰€æœ‰ subsets
        combined_dataset = concatenate_datasets(subsets)
        
        # åœ¨åˆå§‹åŒ–æ™‚å°±é™åˆ¶æ¨£æœ¬æ•¸é‡
        if max_samples is not None and max_samples > 0:
            actual_samples = min(max_samples, len(combined_dataset))
            print(f"[INFO] é™åˆ¶æ¨£æœ¬æ•¸é‡ï¼š{len(combined_dataset)} â†’ {actual_samples}", flush=True)
            self.dataset = combined_dataset.select(range(actual_samples))
        else:
            self.dataset = combined_dataset
        
        self.to_tensor = T.ToTensor()

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        item = self.dataset[idx]

        def rgba2rgb(img_RGBA):
            from PIL import Image
            img_RGB = Image.new("RGB", img_RGBA.size, (128, 128, 128))
            img_RGB.paste(img_RGBA, mask=img_RGBA.split()[3])
            return img_RGB

        def get_img(x):
            from PIL import Image
            if isinstance(x, str):
                img_RGBA = Image.open(x).convert("RGBA")
                img_RGB = rgba2rgb(img_RGBA)
            else:
                img_RGBA = x.convert("RGBA")
                img_RGB = rgba2rgb(img_RGBA)
            return img_RGBA, img_RGB

        whole_img_RGBA, whole_img_RGB = get_img(item["whole_image"])
        whole_cap = item["whole_caption"]
        W, H = whole_img_RGBA.size
        base_layout = [0, 0, W - 1, H - 1]

        layer_image_RGBA = [self.to_tensor(whole_img_RGBA)]
        layer_image_RGB  = [self.to_tensor(whole_img_RGB)]
        layout = [base_layout]

        base_img_RGBA, base_img_RGB = get_img(item["base_image"])
        layer_image_RGBA.append(self.to_tensor(base_img_RGBA))
        layer_image_RGB.append(self.to_tensor(base_img_RGB))
        layout.append(base_layout)

        layer_count = item["layer_count"]
        for i in range(layer_count):
            key = f"layer_{i:02d}"
            img_RGBA, img_RGB = get_img(item[key])
            
            w0, h0, w1, h1 = item[f"{key}_box"]

            canvas_RGBA = Image.new("RGBA", (W, H), (0, 0, 0, 0))
            canvas_RGB = Image.new("RGB", (W, H), (128, 128, 128))

            W_img, H_img = w1 - w0, h1 - h0
            if img_RGBA.size != (W_img, H_img):
                img_RGBA = img_RGBA.resize((W_img, H_img), Image.BILINEAR)
                img_RGB  = img_RGB.resize((W_img, H_img), Image.BILINEAR)

            canvas_RGBA.paste(img_RGBA, (w0, h0), img_RGBA)
            canvas_RGB.paste(img_RGB, (w0, h0))

            layer_image_RGBA.append(self.to_tensor(canvas_RGBA))
            layer_image_RGB.append(self.to_tensor(canvas_RGB))
            layout.append([w0, h0, w1, h1])

        return {
            "pixel_RGBA": layer_image_RGBA,
            "pixel_RGB": layer_image_RGB,
            "whole_img": whole_img_RGB,
            "caption": whole_cap,
            "height": H,
            "width": W,
            "layout": layout,
        }


def inference_layout_limited(config, max_samples: int = 5):
    """
    ä¿®æ”¹ç‰ˆçš„ inference_layoutï¼Œåªè™•ç†å‰ max_samples å€‹æ¨£æœ¬
    """
    import torch  # ç¢ºä¿ torch å·²å°å…¥
    
    if config.get('seed') is not None:
        infer_module.seed_everything(config['seed'])
    
    os.makedirs(config['save_dir'], exist_ok=True)
    os.makedirs(os.path.join(config['save_dir'], "merged"), exist_ok=True)
    os.makedirs(os.path.join(config['save_dir'], "merged_rgba"), exist_ok=True)

    # Load transparent VAEï¼ˆä½¿ç”¨åŸç‰ˆé‚è¼¯ï¼‰
    print("[INFO] Loading Transparent VAE...", flush=True)
    
    # æª¢æŸ¥ CUDA æ˜¯å¦å¯ç”¨
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    if not torch.cuda.is_available():
        print("[WARNING] CUDA is not available, using CPU (this will be very slow!)", flush=True)
    else:
        print(f"[INFO] Using device: {device}", flush=True)
    
    import argparse as argparse_module
    from models.transp_vae import AutoencoderKLTransformerTraining as CustomVAE
    
    vae_args = argparse_module.Namespace(
        max_layers=config.get('max_layers', 48),
        decoder_arch=config.get('decoder_arch', 'vit'),
        pos_embedding=config.get('pos_embedding', 'rope'),
        layer_embedding=config.get('layer_embedding', 'rope'),
        single_layer_decoder=config.get('single_layer_decoder', None)
    )
    transp_vae = CustomVAE(vae_args)
    transp_vae_path = config.get('transp_vae_path')
    
    # ä½¿ç”¨æ­£ç¢ºçš„è¨­å‚™åŠ è¼‰ï¼Œä¸¦æ˜ç¢ºæŒ‡å®š weights_only=Falseï¼ˆå› ç‚º checkpoint å¯èƒ½åŒ…å«éæ¨™æº–å°è±¡ï¼‰
    try:
        transp_vae_weights = torch.load(
            transp_vae_path, 
            map_location=device,
            weights_only=False  # CLD checkpoints å¯èƒ½åŒ…å« argparse.Namespace ç­‰éæ¨™æº–å°è±¡
        )
    except Exception as e:
        print(f"[ERROR] Failed to load transparent VAE weights: {e}", flush=True)
        print(f"[INFO] Trying to load on CPU first, then move to {device}...", flush=True)
        # å¦‚æœç›´æ¥åŠ è¼‰å¤±æ•—ï¼Œå…ˆåŠ è¼‰åˆ° CPUï¼Œç„¶å¾Œç§»å‹•åˆ°ç›®æ¨™è¨­å‚™
        transp_vae_weights = torch.load(
            transp_vae_path,
            map_location="cpu",
            weights_only=False
        )
        # ç§»å‹•æ¬Šé‡åˆ°ç›®æ¨™è¨­å‚™ï¼ˆå¦‚æœéœ€è¦çš„è©±ï¼‰
        if isinstance(transp_vae_weights, dict) and 'model' in transp_vae_weights:
            for k, v in transp_vae_weights['model'].items():
                if isinstance(v, torch.Tensor) and device.type == "cuda":
                    transp_vae_weights['model'][k] = v.to(device)
    
    missing_keys, unexpected_keys = transp_vae.load_state_dict(transp_vae_weights['model'], strict=False)
    if missing_keys:
        print(f"[WARNING] Missing keys: {missing_keys}", flush=True)
    if unexpected_keys:
        print(f"[WARNING] Unexpected keys: {unexpected_keys}", flush=True)
    transp_vae.eval()
    transp_vae = transp_vae.to(device)
    print("[INFO] Transparent VAE loaded successfully.", flush=True)

    # æ‡‰ç”¨ skip_fuse_lora patchï¼ˆå¦‚æœé…ç½®ä¸­å•Ÿç”¨ï¼‰
    apply_skip_fuse_lora_patch(config)
    
    # åˆå§‹åŒ– pipelineï¼ˆä½¿ç”¨åŸç‰ˆé‚è¼¯ï¼‰
    pipeline = infer_module.initialize_pipeline(config)

    # å‰µå»º datasetï¼ˆä½¿ç”¨ä¿®æ”¹ç‰ˆï¼Œåœ¨åˆå§‹åŒ–æ™‚å°±é™åˆ¶æ¨£æœ¬æ•¸é‡ï¼‰
    print(f"[INFO] å‰µå»º datasetï¼ˆå°‡é™åˆ¶ç‚ºå‰ {max_samples} å€‹æ¨£æœ¬ï¼‰...", flush=True)
    
    # ä½¿ç”¨ä¿®æ”¹ç‰ˆçš„ datasetï¼Œåœ¨ split ä¹‹å¾Œç«‹å³é™åˆ¶æ¨£æœ¬æ•¸é‡
    dataset = LimitedLayoutTrainDataset(config['data_dir'], split="test", max_samples=max_samples)
    
    loader = infer_module.DataLoader(
        dataset, 
        batch_size=1, 
        shuffle=False, 
        num_workers=0, 
        collate_fn=infer_module.collate_fn
    )

    generator = torch.Generator(device=device).manual_seed(config.get('seed', 42))

    idx = 0
    for batch in loader:
        print(f"\n{'='*60}")
        print(f"Processing case {idx} (æ¨£æœ¬ {idx+1}/{actual_samples})")
        print(f"{'='*60}", flush=True)

        height = int(batch["height"][0])
        width = int(batch["width"][0])
        adapter_img = batch["whole_img"][0]
        caption = batch["caption"][0]
        layer_boxes = infer_module.get_input_box(batch["layout"][0]) 

        # Debug: é¡¯ç¤º layout è³‡è¨Š
        print(f"[DEBUG] Image size: {width}x{height}", flush=True)
        print(f"[DEBUG] Layout boxes count: {len(layer_boxes)}", flush=True)
        if len(caption) > 100:
            print(f"[DEBUG] Caption: {caption[:100]}...", flush=True)
        else:
            print(f"[DEBUG] Caption: {caption}", flush=True)

        # Generate layers using pipelineï¼ˆä½¿ç”¨åŸç‰ˆé‚è¼¯ï¼‰
        x_hat, image, latents = pipeline(
            prompt=caption,
            adapter_image=adapter_img,
            adapter_conditioning_scale=0.9,
            validation_box=layer_boxes,
            generator=generator,
            height=height,
            width=width,
            guidance_scale=config.get('cfg', 4.0),
            num_layers=len(layer_boxes),
            sdxl_vae=transp_vae,  # Use transparent VAE
        )

        # Adjust x_hat range from [-1, 1] to [0, 1]
        x_hat = (x_hat + 1) / 2

        # Remove batch dimension and ensure float32 dtype
        x_hat = x_hat.squeeze(0).permute(1, 0, 2, 3).to(torch.float32)
        
        this_index = f"case_{idx}"
        case_dir = os.path.join(config['save_dir'], this_index)
        os.makedirs(case_dir, exist_ok=True)
        
        # Save whole image_RGBA (X_hat[0]) and background_RGBA (X_hat[1])
        whole_image_layer = (x_hat[0].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)
        whole_image_rgba_image = Image.fromarray(whole_image_layer, "RGBA")
        whole_image_rgba_image.save(os.path.join(case_dir, "whole_image_rgba.png"))

        adapter_img.save(os.path.join(case_dir, "origin.png"))

        background_layer = (x_hat[1].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)
        background_rgba_image = Image.fromarray(background_layer, "RGBA")
        background_rgba_image.save(os.path.join(case_dir, "background_rgba.png"))

        x_hat = x_hat[2:]
        merged_image = image[1]
        image = image[2:]

        # Save transparent VAE decoded resultsï¼ˆæ·»åŠ  alpha channel è¨ºæ–·ï¼‰
        print(f"[DEBUG] Saving {x_hat.shape[0]} foreground layers...", flush=True)
        for layer_idx in range(x_hat.shape[0]):
            layer = x_hat[layer_idx]
            rgba_layer = (layer.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)
            
            # Debug: æª¢æŸ¥ alpha channel
            alpha_channel = rgba_layer[:, :, 3]
            alpha_min, alpha_max = int(alpha_channel.min()), int(alpha_channel.max())
            alpha_mean = float(alpha_channel.mean())
            transparent_pixels = int((alpha_channel == 0).sum())
            total_pixels = alpha_channel.size
            transparent_ratio = transparent_pixels / total_pixels * 100
            
            # ç²å–å°æ‡‰çš„ box
            if layer_idx < len(layer_boxes) - 2:
                corresponding_box = layer_boxes[layer_idx + 2]
                x1, y1, x2, y2 = corresponding_box
                box_area = (x2 - x1) * (y2 - y1)
                print(f"  Layer {layer_idx}: box={corresponding_box}, box_area={box_area}, "
                      f"alpha_range=[{alpha_min}, {alpha_max}], alpha_mean={alpha_mean:.1f}, "
                      f"transparent={transparent_ratio:.1f}%", flush=True)
            else:
                print(f"  Layer {layer_idx}: alpha_range=[{alpha_min}, {alpha_max}], "
                      f"alpha_mean={alpha_mean:.1f}, transparent={transparent_ratio:.1f}%", flush=True)
            
            rgba_image = Image.fromarray(rgba_layer, "RGBA")
            rgba_image.save(os.path.join(case_dir, f"layer_{layer_idx}_rgba.png"))

        # Composite background and foreground layers
        for layer_idx in range(x_hat.shape[0]):
            rgba_layer = (x_hat[layer_idx].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)
            layer_image = Image.fromarray(rgba_layer, "RGBA")
            merged_image = Image.alpha_composite(merged_image.convert('RGBA'), layer_image)
        
        # Save final composite images
        merged_image.convert('RGB').save(os.path.join(config['save_dir'], "merged", f"{this_index}.png"))
        merged_image.convert('RGB').save(os.path.join(case_dir, f"{this_index}.png"))
        # Save final composite RGBA image
        merged_image.save(os.path.join(config['save_dir'], "merged_rgba", f"{this_index}.png"))

        print(f"âœ… Saved case {idx} to {case_dir}")
        idx += 1

    del pipeline
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    print(f"\n{'='*60}")
    print(f"âœ… æ¸¬è©¦å®Œæˆï¼è™•ç†äº† {idx} å€‹æ¨£æœ¬")
    print(f"   è¼¸å‡ºç›®éŒ„ï¼š{config['save_dir']}")
    print(f"{'='*60}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description="ä½¿ç”¨åŸç‰ˆ CLD infer.py æ¸¬è©¦ï¼Œä½†åªè™•ç†å°‘é‡æ¨£æœ¬"
    )
    parser.add_argument(
        "--config_path", "-c", 
        type=str, 
        required=True, 
        help="Path to the YAML configuration file."
    )
    parser.add_argument(
        "--max_samples", "-n",
        type=int,
        default=5,
        help="æœ€å¤šè™•ç†çš„æ¨£æœ¬æ•¸é‡ï¼ˆé è¨­ï¼š5ï¼‰"
    )
    args = parser.parse_args()

    # å°å…¥å¿…è¦çš„æ¨¡çµ„ï¼ˆtorch å·²ç¶“åœ¨é ‚éƒ¨å°å…¥ï¼‰
    import numpy as np
    from PIL import Image

    # CUDA æª¢æŸ¥å·²ç¶“åœ¨é ‚éƒ¨å®Œæˆï¼Œé€™è£¡ç›´æ¥åŠ è¼‰é…ç½®
    config = infer_module.load_config(args.config_path)
    
    # æ‡‰ç”¨ skip_fuse_lora patchï¼ˆå¦‚æœé…ç½®ä¸­å•Ÿç”¨ï¼‰
    # é€™éœ€è¦åœ¨åŠ è¼‰ config ä¹‹å¾Œï¼Œä½†åœ¨ initialize_pipeline ä¹‹å‰
    apply_skip_fuse_lora_patch(config)
    
    print(f"\n{'='*60}")
    print("CLD æ¸¬è©¦è…³æœ¬ï¼ˆé™åˆ¶æ¨£æœ¬æ•¸é‡ï¼‰")
    print(f"{'='*60}")
    print(f"é…ç½®æª”æ¡ˆï¼š{args.config_path}")
    print(f"æœ€å¤šè™•ç†æ¨£æœ¬æ•¸ï¼š{args.max_samples}")
    print(f"è¼¸å‡ºç›®éŒ„ï¼š{config['save_dir']}")
    if config.get('skip_fuse_lora', False):
        print(f"è¨˜æ†¶é«”å„ªåŒ–ï¼šskip_fuse_lora=True (è·³é fuse_lora ä»¥ç¯€çœè¨˜æ†¶é«”)")
    print(f"{'='*60}\n")
    print("âš ï¸  æ³¨æ„ï¼šé¦–æ¬¡é‹è¡Œæ™‚æœƒä¸‹è¼‰ PrismLayersPro dataset çš„ metadata")
    print("   ä½†åªæœƒè™•ç†å‰ {} å€‹æ¨£æœ¬ï¼Œä¸æœƒä¸‹è¼‰æ•´å€‹ 100GB+ dataset\n".format(args.max_samples))

    try:
        inference_layout_limited(config, max_samples=args.max_samples)
        raise SystemExit(0)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ¶ä¸­æ–·")
        raise SystemExit(130)
    except SystemExit:
        raise
    except Exception as e:
        print(f"\n\nâŒ éŒ¯èª¤ï¼š{e}", flush=True)
        import traceback
        traceback.print_exc()
        raise SystemExit(1)

