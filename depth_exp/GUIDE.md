# Depth Experiments Guide

æœ¬æŒ‡å—èªªæ˜å¦‚ä½•ä½¿ç”¨ `depth_exp` ç›®éŒ„ä¸­çš„å·¥å…·é€²è¡Œæ·±åº¦ç›¸é—œçš„å¯¦é©—ã€‚

## ğŸ“ ç›®éŒ„çµæ§‹

```
depth_exp/
â”œâ”€â”€ GUIDE.md                    # æœ¬æŒ‡å—
â”œâ”€â”€ caption_llava15.json         # LLaVA ç”Ÿæˆçš„åœ–åƒ caption
â”œâ”€â”€ CLD/                         # Controllable Layer Decomposition
â”‚   â”œâ”€â”€ train/                   # è¨“ç·´ç›¸é—œ
â”‚   â”‚   â”œâ”€â”€ train.py             # åŸå§‹ CLD è¨“ç·´è…³æœ¬
â”‚   â”‚   â”œâ”€â”€ train.yaml           # åŸå§‹è¨“ç·´é…ç½®
â”‚   â”‚   â”œâ”€â”€ train_dlcv.py        # DLCV æ•¸æ“šé›†è¨“ç·´è…³æœ¬ï¼ˆåƒ…è¨“ç·´ MLCAï¼‰
â”‚   â”‚   â””â”€â”€ train_dlcv.yaml     # DLCV è¨“ç·´é…ç½®
â”‚   â”œâ”€â”€ infer/                   # æ¨ç†ç›¸é—œ
â”‚   â”œâ”€â”€ eval/                    # è©•ä¼°ç›¸é—œ
â”‚   â””â”€â”€ models/                  # æ¨¡å‹å®šç¾©
â””â”€â”€ ml-depth-pro/                # Depth Pro æ·±åº¦ä¼°è¨ˆæ¨¡å‹
    â”œâ”€â”€ src/depth_pro/           # Depth Pro æ ¸å¿ƒä»£ç¢¼
    â””â”€â”€ get_pretrained_models.sh # ä¸‹è¼‰é è¨“ç·´æ¨¡å‹
```

## ğŸš€ å¿«é€Ÿé–‹å§‹

### 1. ç’°å¢ƒè¨­ç½®

#### CLD ç’°å¢ƒ
```bash
cd depth_exp/CLD
conda env create -f environment.yml
conda activate CLD
```

#### Depth Pro ç’°å¢ƒï¼ˆå¯é¸ï¼Œç”¨æ–¼æ·±åº¦ channelï¼‰
```bash
cd depth_exp/ml-depth-pro
conda create -n depth-pro -y python=3.9
conda activate depth-pro
pip install -e .
source get_pretrained_models.sh  # ä¸‹è¼‰é è¨“ç·´æ¨¡å‹
```

### 2. æº–å‚™æ¨¡å‹æ¬Šé‡

#### ä¸‹è¼‰ FLUX.1-dev æ¨¡å‹
```python
from huggingface_hub import snapshot_download

repo_id = "black-forest-labs/FLUX.1-dev"
snapshot_download(repo_id, local_dir="path/to/FLUX.1-dev")
```

#### ä¸‹è¼‰ Adapter æ¬Šé‡
```python
repo_id = "alimama-creative/FLUX.1-dev-Controlnet-Inpainting-Alpha"
snapshot_download(repo_id, local_dir="path/to/adapter")
```

#### ä¸‹è¼‰ CLD LoRA æ¬Šé‡ï¼ˆå¾ HuggingFaceï¼‰
è¨ªå• https://huggingface.co/thuteam/CLD ä¸‹è¼‰ä»¥ä¸‹æ–‡ä»¶ï¼š
```
ckpt/
â”œâ”€â”€ decouple_LoRA/
â”‚   â”œâ”€â”€ adapter/
â”‚   â”‚   â””â”€â”€ pytorch_lora_weights.safetensors
â”‚   â”œâ”€â”€ layer_pe.pth
â”‚   â””â”€â”€ transformer/
â”‚       â””â”€â”€ pytorch_lora_weights.safetensors
â”œâ”€â”€ pre_trained_LoRA/
â”‚   â””â”€â”€ pytorch_lora_weights.safetensors
â””â”€â”€ prism_ft_LoRA/
    â””â”€â”€ pytorch_lora_weights.safetensors
```

## ğŸ“Š ä½¿ç”¨ DLCV æ•¸æ“šé›†è¨“ç·´ CLD

### æ¦‚è¿°

`train_dlcv.py` æ˜¯å°ˆé–€ç‚º DLCV æ•¸æ“šé›†è¨­è¨ˆçš„è¨“ç·´è…³æœ¬ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹é»ï¼š

- âœ… ä½¿ç”¨ `DLCVCLDDataset` å¾ HuggingFace è¼‰å…¥æ•¸æ“š
- âœ… è‡ªå‹•å¾ `caption_llava15.json` è®€å– caption
- âœ… å¯é¸å•Ÿç”¨æ·±åº¦ channelï¼ˆä½¿ç”¨ ml-depth-proï¼‰
- âœ… **åƒ…è¨“ç·´ MultiLayer-Adapter (MLCA)**ï¼ŒTransformer å®Œå…¨å‡çµ

### é…ç½®è¨“ç·´

ç·¨è¼¯ `depth_exp/CLD/train/train_dlcv.yaml`ï¼š

```yaml
# åŸºæœ¬é…ç½®
seed: 42
max_layer_num: 52
max_steps: 200000
log_every: 1000
save_every: 1000
accum_steps: 4

# LoRA é…ç½®
lora_rank: 64
lora_alpha: 64
lora_dropout: 0

# æ•¸æ“šé›†é…ç½®
train_max_samples: null  # null è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰æ¨£æœ¬ï¼Œæˆ–æŒ‡å®šæ•¸é‡å¦‚ 20000
dataset_seed: 42
shuffle_buffer_size: 2000
caption_json_path: null  # null ä½¿ç”¨é»˜èªè·¯å¾‘ (depth_exp/caption_llava15.json)

# æ·±åº¦ channelï¼ˆå¯é¸ï¼‰
use_depth: false  # è¨­ç‚º true å•Ÿç”¨æ·±åº¦ channel
depth_device: null  # null è‡ªå‹•é¸æ“‡ (cuda/cpu)

# æ¨¡å‹è·¯å¾‘
pretrained_model_name_or_path: "path/to/FLUX.1-dev"
pretrained_adapter_path: "path/to/adapter"
pretrained_lora_dir: "path/to/pre_trained_LoRA"  # å¯é¸
artplus_lora_dir: null  # å¯é¸

# è¼¸å‡º
output_dir: "path/to/save/checkpoints"
resume_from: null  # å¯é¸ï¼šå¾ checkpoint æ¢å¾©è¨“ç·´
```

### é–‹å§‹è¨“ç·´

```bash
cd depth_exp/CLD/train
conda activate CLD
python train_dlcv.py -c train_dlcv.yaml
```

### è¨“ç·´é¸é …èªªæ˜

#### 1. åŸºæœ¬è¨“ç·´ï¼ˆä¸ä½¿ç”¨æ·±åº¦ï¼‰
```yaml
use_depth: false
train_max_samples: 20000  # é™åˆ¶æ¨£æœ¬æ•¸é‡
```

#### 2. ä½¿ç”¨æ·±åº¦ channel è¨“ç·´
```yaml
use_depth: true
depth_device: "cuda"  # æˆ– "cpu"
```

**æ³¨æ„**ï¼šä½¿ç”¨æ·±åº¦ channel éœ€è¦ï¼š
- å·²å®‰è£ ml-depth-pro
- å·²ä¸‹è¼‰ Depth Pro é è¨“ç·´æ¨¡å‹
- æ›´å¤š GPU è¨˜æ†¶é«”

#### 3. å¾ checkpoint æ¢å¾©
```yaml
resume_from: "path/to/checkpoint/directory"
```

Checkpoint ç›®éŒ„çµæ§‹ï¼š
```
checkpoint_dir/
â”œâ”€â”€ adapter/
â”‚   â”œâ”€â”€ pytorch_lora_weights.safetensors
â”‚   â”œâ”€â”€ optimizer.bin
â”‚   â””â”€â”€ scheduler.bin
â”œâ”€â”€ transformer/  # ç©ºç›®éŒ„ï¼ˆMLCA-only è¨“ç·´ï¼‰
â””â”€â”€ layer_pe.pth
```

### ç›£æ§è¨“ç·´

è¨“ç·´éç¨‹ä¸­æœƒï¼š
- åœ¨ `output_dir` ä¿å­˜ checkpoint
- åœ¨ TensorBoard è¨˜éŒ„ lossï¼ˆ`tensorboard --logdir output_dir`ï¼‰
- åœ¨çµ‚ç«¯é¡¯ç¤ºé€²åº¦æ¢å’Œ loss

## ğŸ” æ•¸æ“šé›†èªªæ˜

### DLCVCLDDataset

ä½ç½®ï¼š`src/data/dlcv_cld_dataset.py`

**åŠŸèƒ½**ï¼š
- å¾ HuggingFace è¼‰å…¥ `WalkerHsu/DLCV2025_final_project_piccollage` æ•¸æ“šé›†
- è‡ªå‹•è™•ç†åœ–åƒå±¤ï¼ˆlayersï¼‰å’Œé‚Šç•Œæ¡†ï¼ˆbounding boxesï¼‰
- æ”¯æ´æ—‹è½‰å’Œ alpha crop è™•ç†
- å¾ `caption_llava15.json` è®€å– caption
- å¯é¸æ·»åŠ æ·±åº¦ channel

**æ•¸æ“šæ ¼å¼**ï¼š
- `pixel_RGBA`: æ¯å€‹ layer çš„ RGBA tensor åˆ—è¡¨
- `pixel_RGB`: æ¯å€‹ layer çš„ RGB tensor åˆ—è¡¨
- `whole_img`: å®Œæ•´åœ–åƒçš„ RGB PIL Image
- `caption`: æ–‡å­—æè¿°
- `layout`: é‚Šç•Œæ¡†åˆ—è¡¨ `[[x1, y1, x2, y2], ...]`

### Caption æ–‡ä»¶

`caption_llava15.json` æ ¼å¼ï¼š
```json
{
  "/path/to/image/00000000.png": "Caption text here...",
  "/path/to/image/00000001.png": "Another caption...",
  ...
}
```

Dataset æœƒæ ¹æ“šåœ–åƒ ID è‡ªå‹•åŒ¹é… captionã€‚

## ğŸ¯ è¨“ç·´ç­–ç•¥

### MLCA-Only è¨“ç·´

`train_dlcv.py` å°ˆé–€è¨­è¨ˆç‚º**åƒ…è¨“ç·´ MultiLayer-Adapter**ï¼š

- âœ… Transformer å®Œå…¨å‡çµï¼ˆ`requires_grad=False`, `eval()` æ¨¡å¼ï¼‰
- âœ… åƒ…è¨“ç·´ Adapter çš„ LoRA æ¬Šé‡å’Œ layer_pe
- âœ… æ›´å¿«çš„è¨“ç·´é€Ÿåº¦
- âœ… æ›´å°‘çš„è¨˜æ†¶é«”ä½¿ç”¨
- âœ… é©åˆåœ¨ DLCV æ•¸æ“šé›†ä¸Šå¾®èª¿

### èˆ‡åŸå§‹è¨“ç·´çš„å€åˆ¥

| ç‰¹æ€§ | `train.py` (åŸå§‹) | `train_dlcv.py` (DLCV) |
|------|------------------|------------------------|
| æ•¸æ“šé›† | PrismLayersPro | DLCV (HuggingFace) |
| Caption | æ•¸æ“šé›†å…§å»º | å¾ JSON æ–‡ä»¶è®€å– |
| æ·±åº¦ channel | âŒ | âœ… å¯é¸ |
| è¨“ç·´ç›®æ¨™ | Transformer + MLCA | åƒ… MLCA |
| Transformer ç‹€æ…‹ | å¯è¨“ç·´ | å®Œå…¨å‡çµ |

## ğŸ”§ æ•…éšœæ’é™¤

### å•é¡Œ 1: æ‰¾ä¸åˆ° depth_pro æ¨¡çµ„

**éŒ¯èª¤**ï¼š
```
ImportError: depth_pro is not available
```

**è§£æ±º**ï¼š
```bash
cd depth_exp/ml-depth-pro
pip install -e .
source get_pretrained_models.sh
```

æˆ–è¨­ç½® `use_depth: false` ä¸ä½¿ç”¨æ·±åº¦ channelã€‚

### å•é¡Œ 2: CUDA è¨˜æ†¶é«”ä¸è¶³

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
1. æ¸›å°‘ `train_max_samples`
2. å¢åŠ  `accum_steps`ï¼ˆæ¢¯åº¦ç´¯ç©ï¼‰
3. è¨­ç½® `use_depth: false`
4. ä½¿ç”¨æ›´å°çš„ `lora_rank`

### å•é¡Œ 3: Caption æ‰¾ä¸åˆ°

**æª¢æŸ¥**ï¼š
- `caption_llava15.json` æ˜¯å¦å­˜åœ¨æ–¼ `depth_exp/` ç›®éŒ„
- åœ–åƒ ID æ ¼å¼æ˜¯å¦åŒ¹é…ï¼ˆdataset æœƒè‡ªå‹•è™•ç†ä¸åŒæ ¼å¼ï¼‰

### å•é¡Œ 4: æ•¸æ“šé›†è¼‰å…¥ç·©æ…¢

**å„ªåŒ–**ï¼š
- è¨­ç½® `train_max_samples` é™åˆ¶æ¨£æœ¬æ•¸é‡
- èª¿æ•´ `shuffle_buffer_size`
- ä½¿ç”¨æœ¬åœ°ç·©å­˜çš„ HuggingFace æ•¸æ“šé›†

## ğŸ“ˆ è©•ä¼°è¨“ç·´çµæœ

### æª¢æŸ¥ Checkpoint

è¨“ç·´å¾Œï¼Œcheckpoint ä¿å­˜åœ¨ `output_dir`ï¼š
```
output_dir/
â”œâ”€â”€ adapter/
â”‚   â”œâ”€â”€ pytorch_lora_weights.safetensors  # MLCA LoRA æ¬Šé‡
â”‚   â”œâ”€â”€ optimizer.bin
â”‚   â””â”€â”€ scheduler.bin
â”œâ”€â”€ transformer/  # ç©ºç›®éŒ„ï¼ˆMLCA-onlyï¼‰
â”œâ”€â”€ layer_pe.pth  # Layer positional encoding
â””â”€â”€ random_states_0.pkl  # RNG ç‹€æ…‹
```

### ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹

åœ¨ `infer/infer.yaml` ä¸­è¨­ç½®ï¼š
```yaml
adapter_lora_dir: "path/to/output_dir/adapter"
layer_ckpt: "path/to/output_dir"
```

ç„¶å¾Œé‹è¡Œæ¨ç†ï¼š
```bash
cd depth_exp/CLD/infer
python infer.py -c infer.yaml
```

## ğŸ“ æœ€ä½³å¯¦è¸

1. **é–‹å§‹æ™‚ä½¿ç”¨å°æ¨£æœ¬**ï¼š
   ```yaml
   train_max_samples: 1000
   max_steps: 5000
   ```

2. **é€æ­¥å¢åŠ è¦æ¨¡**ï¼š
   - å…ˆç”¨å°æ¨£æœ¬é©—è­‰æµç¨‹
   - ç¢ºèª loss æ­£å¸¸ä¸‹é™
   - å†ä½¿ç”¨å®Œæ•´æ•¸æ“šé›†

3. **ç›£æ§è³‡æºä½¿ç”¨**ï¼š
   - ä½¿ç”¨ `nvidia-smi` ç›£æ§ GPU
   - ä½¿ç”¨ TensorBoard ç›£æ§ loss

4. **å®šæœŸä¿å­˜**ï¼š
   - è¨­ç½®åˆç†çš„ `save_every`
   - é‡è¦ checkpoint æ‰‹å‹•å‚™ä»½

5. **å¯¦é©—è¨˜éŒ„**ï¼š
   - è¨˜éŒ„ä½¿ç”¨çš„é…ç½®åƒæ•¸
   - è¨˜éŒ„è¨“ç·´éç¨‹ä¸­çš„è§€å¯Ÿ
   - ä¿å­˜é‡è¦çš„å¯¦é©—çµæœ

## ğŸ”— ç›¸é—œè³‡æº

- **CLD è«–æ–‡**: https://arxiv.org/abs/2511.16249
- **CLD HuggingFace**: https://huggingface.co/thuteam/CLD
- **Depth Pro è«–æ–‡**: https://arxiv.org/abs/2410.02073
- **FLUX.1-dev**: https://huggingface.co/black-forest-labs/FLUX.1-dev

## ğŸ’¡ é€²éšç”¨æ³•

### è‡ªå®šç¾© Caption è·¯å¾‘

```yaml
caption_json_path: "path/to/custom_caption.json"
```

### æ··åˆä½¿ç”¨æ·±åº¦å’ŒåŸå§‹æ•¸æ“š

å¯ä»¥åˆ†åˆ¥è¨“ç·´å…©å€‹æ¨¡å‹ï¼š
1. ä¸ä½¿ç”¨æ·±åº¦çš„æ¨¡å‹ï¼ˆ`use_depth: false`ï¼‰
2. ä½¿ç”¨æ·±åº¦çš„æ¨¡å‹ï¼ˆ`use_depth: true`ï¼‰

ç„¶å¾Œæ¯”è¼ƒçµæœã€‚

### èª¿æ•´ LoRA åƒæ•¸

æ ¹æ“šæ•¸æ“šé›†å¤§å°èª¿æ•´ï¼š
```yaml
# å°æ•¸æ“šé›†
lora_rank: 32
lora_alpha: 32

# å¤§æ•¸æ“šé›†
lora_rank: 128
lora_alpha: 128
```

## â“ å¸¸è¦‹å•é¡Œ

**Q: ç‚ºä»€éº¼åªè¨“ç·´ MLCAï¼Ÿ**  
A: MLCA æ˜¯æ§åˆ¶å±¤åˆ†è§£çš„æ ¸å¿ƒçµ„ä»¶ï¼Œåœ¨ DLCV æ•¸æ“šé›†ä¸Šå¾®èª¿ MLCA é€šå¸¸è¶³å¤ ï¼Œä¸”è¨“ç·´æ›´å¿«ã€æ›´ç©©å®šã€‚

**Q: å¯ä»¥ä½¿ç”¨åŸå§‹ `train.py` è¨“ç·´ DLCV æ•¸æ“šé›†å—ï¼Ÿ**  
A: å¯ä»¥ï¼Œä½†éœ€è¦ä¿®æ”¹ dataset å°å…¥ã€‚`train_dlcv.py` å·²ç¶“æ•´åˆäº† DLCV æ•¸æ“šé›†å’Œç›¸é—œåŠŸèƒ½ã€‚

**Q: æ·±åº¦ channel æ˜¯å¿…éœ€çš„å—ï¼Ÿ**  
A: ä¸æ˜¯ã€‚æ·±åº¦ channel æ˜¯å¯é¸åŠŸèƒ½ï¼Œå¯ä»¥å¹«åŠ©æ¨¡å‹ç†è§£å ´æ™¯æ·±åº¦ï¼Œä½†æ¨™æº–è¨“ç·´ä¸éœ€è¦ã€‚

**Q: å¦‚ä½•çŸ¥é“è¨“ç·´æ˜¯å¦æ­£å¸¸ï¼Ÿ**  
A: è§€å¯Ÿï¼š
- Loss æ‡‰è©²é€æ¼¸ä¸‹é™
- TensorBoard æ›²ç·šæ‡‰è©²å¹³æ»‘
- æ²’æœ‰ CUDA éŒ¯èª¤æˆ–è¨˜æ†¶é«”å•é¡Œ

---

**æœ€å¾Œæ›´æ–°**: 2025-01-XX  
**ç¶­è­·è€…**: DLCV Final Project Team

