# CLD Inference Configuration for Pipeline-generated data
# This config uses JSON files generated by pipeline_to_cld_infer.py

seed: 42
max_layer_num: 52

# Use pipeline-generated JSON files instead of HuggingFace dataset
use_pipeline_dataset: true

# Path to directory containing JSON files from pipeline_to_cld_infer.py
# This should point to the output_dir from pipeline_config.yaml's cld section
data_dir: "outputs/pipeline_outputs/cld"

# Reduce VRAM by downscaling large inputs at dataset loading time.
# Recommended for 16GB GPUs. Set to null to disable.
max_image_side: 1024

# Model paths
pretrained_model_name_or_path: "../../checkpoints/flux/FLUX.1-dev"
pretrained_adapter_path: "../../checkpoints/flux/FLUX.1-dev-Controlnet-Inpainting-Alpha"
transp_vae_path: "../../checkpoints/cld/trans_vae/0008000.pt"
pretrained_lora_dir: "../../checkpoints/cld/pre_trained_LoRA"
artplus_lora_dir: "../../checkpoints/cld/prism_ft_LoRA"
lora_ckpt: "../../checkpoints/cld/decouple_LoRA/transformer"
layer_ckpt: "../../checkpoints/cld/decouple_LoRA"
adapter_lora_dir: "../../checkpoints/cld/decouple_LoRA/adapter"

# Output directory for inference results
save_dir: "../../outputs/pipeline_outputs/cld_inference"

# Inference parameters
cfg: 4.0  # Guidance scale
num_inference_steps: 28


