"""
Custom Dataset for CLD inference using pipeline-generated JSON files.
This dataset reads JSON files generated by pipeline_to_cld_infer.py.
"""

import json
import numpy as np
import torch
from PIL import Image, ImageOps
from pathlib import Path
from torch.utils.data import Dataset
import torchvision.transforms as T
from typing import List, Dict, Any, Optional, Tuple, Union


def collate_fn_pipeline(batch):
    """Collate function for pipeline dataset."""
    return {
        "whole_img": [item["whole_img"] for item in batch],
        "caption": [item["caption"] for item in batch],
        "height": [item["height"] for item in batch],
        "width": [item["width"] for item in batch],
        "layout": [item["layout"] for item in batch],
    }


class PipelineDataset(Dataset):
    """
    Dataset that reads JSON files generated by pipeline_to_cld_infer.py.
    
    Each JSON file should contain:
    - image_path: path to the original image
    - ordered_bboxes: list of [x1, y1, x2, y2] boxes
    - quantized_boxes: list of quantized boxes
    - layer_indices: list of layer indices (1-based foreground layers)
    - caption: original caption
    - whole_caption: caption generated in Step 3.5 (or fallback to caption)
    """
    
    def __init__(
        self,
        data_dir: str,
        max_image_side: Optional[int] = None,
        max_image_size: Optional[Union[List[int], Tuple[int, int]]] = None,
    ):
        """
        Initialize dataset from directory containing JSON files.
        
        Args:
            data_dir: Directory containing JSON files from pipeline_to_cld_infer.py
            max_image_side: If set, downscale images so max(width, height) <= max_image_side.
                            DISABLED: Commented out to preserve image quality.
            max_image_size: If set to (max_w, max_h) (or [max_w, max_h]), downscale images to fit within it.
        """
        self.data_dir = Path(data_dir)
        self.json_files = sorted(list(self.data_dir.glob("*.json")))
        # DISABLED: max_image_side to preserve image quality
        # self.max_image_side = int(max_image_side) if max_image_side is not None else None
        self.max_image_side = None

        if max_image_size is None:
            self.max_image_size: Optional[Tuple[int, int]] = None
        else:
            if isinstance(max_image_size, (list, tuple)) and len(max_image_size) == 2:
                self.max_image_size = (int(max_image_size[0]), int(max_image_size[1]))
            else:
                raise ValueError(f"max_image_size must be a 2-tuple/list (w,h), got: {max_image_size!r}")
        
        if len(self.json_files) == 0:
            raise ValueError(f"No JSON files found in {data_dir}")
        
        print(f"[INFO] Found {len(self.json_files)} JSON files in {data_dir}")
        self.to_tensor = T.ToTensor()
    
    def __len__(self):
        return len(self.json_files)
    
    def __getitem__(self, idx):
        """Load a single sample from JSON file."""
        json_path = self.json_files[idx]
        
        # Load JSON data
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # Load image
        image_path = Path(data["image_path"])
        if not image_path.is_absolute():
            # Try relative to JSON file location
            image_path = json_path.parent.parent / image_path
            if not image_path.exists():
                # Try relative to data_dir
                image_path = self.data_dir.parent / data["image_path"]
        
        if not image_path.exists():
            raise FileNotFoundError(f"Image not found: {data['image_path']}")
        
        whole_img = Image.open(image_path).convert("RGB")
        W0, H0 = whole_img.size

        # Optional downscale to reduce VRAM usage during CLD inference.
        # IMPORTANT: scale layout boxes accordingly.
        scale = 1.0
        # DISABLED: max_image_side scaling to preserve image quality
        # if self.max_image_side is not None and max(W0, H0) > self.max_image_side:
        #     scale = min(scale, float(self.max_image_side) / float(max(W0, H0)))
        if self.max_image_size is not None:
            max_w, max_h = self.max_image_size
            if W0 > max_w or H0 > max_h:
                scale = min(scale, float(max_w) / float(W0), float(max_h) / float(H0))

        if scale < 1.0:
            W_res = max(1, int(round(W0 * scale)))
            H_res = max(1, int(round(H0 * scale)))
            whole_img = whole_img.resize((W_res, H_res), resample=Image.Resampling.LANCZOS)
        else:
            W_res, H_res = W0, H0

        # CLD/Flux pipeline logic (and `get_input_box`) quantizes boxes at a stride of 16 pixels.
        # If we only pad to 8, `quantized_max_*` can exceed image size and cause off-by-one
        # token-length mismatches during reshape (see mmdit.fill_in_processed_tokens).
        # So we pad on the right/bottom to the next multiple of 16 to preserve box coordinates.
        def _round_up_to_multiple(x: int, m: int) -> int:
            return ((x + m - 1) // m) * m

        W16 = _round_up_to_multiple(W_res, 16)
        H16 = _round_up_to_multiple(H_res, 16)
        whole_img = whole_img
        if (W16 != W_res) or (H16 != H_res):
            pad_w = W16 - W_res
            pad_h = H16 - H_res
            whole_img = ImageOps.expand(whole_img, border=(0, 0, pad_w, pad_h), fill=(0, 0, 0))
        W, H = whole_img.size
        
        # Get layout boxes (prefer ordered_bboxes; infer.py will quantize to 16 anyway).
        layout_boxes = data.get("ordered_bboxes", data.get("quantized_boxes", []))

        # NOTE: scale boxes based on the resized (pre-padding) resolution; padding is only on right/bottom.
        if scale < 1.0 and layout_boxes:
            sx = float(W_res) / float(W0)
            sy = float(H_res) / float(H0)
            scaled_boxes = []
            for b in layout_boxes:
                x1, y1, x2, y2 = b
                x1n = int(round(float(x1) * sx))
                y1n = int(round(float(y1) * sy))
                x2n = int(round(float(x2) * sx))
                y2n = int(round(float(y2) * sy))

                # Clamp to resized (pre-padding) image bounds.
                x1n = max(0, min(W_res - 1, x1n))
                y1n = max(0, min(H_res - 1, y1n))
                x2n = max(0, min(W_res - 1, x2n))
                y2n = max(0, min(H_res - 1, y2n))

                scaled_boxes.append([x1n, y1n, x2n, y2n])
            layout_boxes = scaled_boxes
        
        # Convert to format expected by infer.py: [x1, y1, x2, y2]
        # Layout should include full image box first, then layer boxes
        layout = [[0, 0, W - 1, H - 1]]  # Full image box (padded size if padding applied)
        layout.extend(layout_boxes)  # Add layer boxes
        
        # Get caption (prefer whole_caption, fallback to caption)
        caption = data.get("whole_caption", data.get("caption", ""))
        
        return {
            "whole_img": whole_img,
            "caption": caption,
            "height": H,
            "width": W,
            "layout": layout,
        }


